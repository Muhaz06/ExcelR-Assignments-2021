{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QbKopJ0rYpjt"
   },
   "source": [
    "# Artificial Neural Networks - 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2003,
     "status": "ok",
     "timestamp": 1641640733181,
     "user": {
      "displayName": "Mohd Muhaz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00454428618184869141"
     },
     "user_tz": -330
    },
    "id": "MuZVwIIA70qd"
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "# Create your first MLP in Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "executionInfo": {
     "elapsed": 460,
     "status": "ok",
     "timestamp": 1641640742149,
     "user": {
      "displayName": "Mohd Muhaz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00454428618184869141"
     },
     "user_tz": -330
    },
    "id": "FwXGBu9c75ci",
    "outputId": "29c21dfd-afef-4e1b-a6f0-92ffe04d8e78"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-56a64c85-4778-458e-9bae-9293e4353541\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>FFMC</th>\n",
       "      <th>DMC</th>\n",
       "      <th>DC</th>\n",
       "      <th>ISI</th>\n",
       "      <th>temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>wind</th>\n",
       "      <th>rain</th>\n",
       "      <th>area</th>\n",
       "      <th>dayfri</th>\n",
       "      <th>daymon</th>\n",
       "      <th>daysat</th>\n",
       "      <th>daysun</th>\n",
       "      <th>daythu</th>\n",
       "      <th>daytue</th>\n",
       "      <th>daywed</th>\n",
       "      <th>monthapr</th>\n",
       "      <th>monthaug</th>\n",
       "      <th>monthdec</th>\n",
       "      <th>monthfeb</th>\n",
       "      <th>monthjan</th>\n",
       "      <th>monthjul</th>\n",
       "      <th>monthjun</th>\n",
       "      <th>monthmar</th>\n",
       "      <th>monthmay</th>\n",
       "      <th>monthnov</th>\n",
       "      <th>monthoct</th>\n",
       "      <th>monthsep</th>\n",
       "      <th>size_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mar</td>\n",
       "      <td>fri</td>\n",
       "      <td>86.2</td>\n",
       "      <td>26.2</td>\n",
       "      <td>94.3</td>\n",
       "      <td>5.1</td>\n",
       "      <td>8.2</td>\n",
       "      <td>51</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>oct</td>\n",
       "      <td>tue</td>\n",
       "      <td>90.6</td>\n",
       "      <td>35.4</td>\n",
       "      <td>669.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>18.0</td>\n",
       "      <td>33</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oct</td>\n",
       "      <td>sat</td>\n",
       "      <td>90.6</td>\n",
       "      <td>43.7</td>\n",
       "      <td>686.9</td>\n",
       "      <td>6.7</td>\n",
       "      <td>14.6</td>\n",
       "      <td>33</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mar</td>\n",
       "      <td>fri</td>\n",
       "      <td>91.7</td>\n",
       "      <td>33.3</td>\n",
       "      <td>77.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>97</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mar</td>\n",
       "      <td>sun</td>\n",
       "      <td>89.3</td>\n",
       "      <td>51.3</td>\n",
       "      <td>102.2</td>\n",
       "      <td>9.6</td>\n",
       "      <td>11.4</td>\n",
       "      <td>99</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>aug</td>\n",
       "      <td>sun</td>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>27.8</td>\n",
       "      <td>32</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>aug</td>\n",
       "      <td>sun</td>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>21.9</td>\n",
       "      <td>71</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>aug</td>\n",
       "      <td>sun</td>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>21.2</td>\n",
       "      <td>70</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>aug</td>\n",
       "      <td>sat</td>\n",
       "      <td>94.4</td>\n",
       "      <td>146.0</td>\n",
       "      <td>614.7</td>\n",
       "      <td>11.3</td>\n",
       "      <td>25.6</td>\n",
       "      <td>42</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>nov</td>\n",
       "      <td>tue</td>\n",
       "      <td>79.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>106.7</td>\n",
       "      <td>1.1</td>\n",
       "      <td>11.8</td>\n",
       "      <td>31</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>517 rows Ã— 31 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-56a64c85-4778-458e-9bae-9293e4353541')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-56a64c85-4778-458e-9bae-9293e4353541 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-56a64c85-4778-458e-9bae-9293e4353541');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "    month  day  FFMC    DMC  ...  monthnov  monthoct  monthsep  size_category\n",
       "0     mar  fri  86.2   26.2  ...         0         0         0          small\n",
       "1     oct  tue  90.6   35.4  ...         0         1         0          small\n",
       "2     oct  sat  90.6   43.7  ...         0         1         0          small\n",
       "3     mar  fri  91.7   33.3  ...         0         0         0          small\n",
       "4     mar  sun  89.3   51.3  ...         0         0         0          small\n",
       "..    ...  ...   ...    ...  ...       ...       ...       ...            ...\n",
       "512   aug  sun  81.6   56.7  ...         0         0         0          large\n",
       "513   aug  sun  81.6   56.7  ...         0         0         0          large\n",
       "514   aug  sun  81.6   56.7  ...         0         0         0          large\n",
       "515   aug  sat  94.4  146.0  ...         0         0         0          small\n",
       "516   nov  tue  79.5    3.0  ...         1         0         0          small\n",
       "\n",
       "[517 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"forestfires.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1641640746222,
     "user": {
      "displayName": "Mohd Muhaz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00454428618184869141"
     },
     "user_tz": -330
    },
    "id": "OSfzSE8Y76Es",
    "outputId": "cc7a71e4-7583-4812-d0a7-c2a977eac06f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 517 entries, 0 to 516\n",
      "Data columns (total 31 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   month          517 non-null    object \n",
      " 1   day            517 non-null    object \n",
      " 2   FFMC           517 non-null    float64\n",
      " 3   DMC            517 non-null    float64\n",
      " 4   DC             517 non-null    float64\n",
      " 5   ISI            517 non-null    float64\n",
      " 6   temp           517 non-null    float64\n",
      " 7   RH             517 non-null    int64  \n",
      " 8   wind           517 non-null    float64\n",
      " 9   rain           517 non-null    float64\n",
      " 10  area           517 non-null    float64\n",
      " 11  dayfri         517 non-null    int64  \n",
      " 12  daymon         517 non-null    int64  \n",
      " 13  daysat         517 non-null    int64  \n",
      " 14  daysun         517 non-null    int64  \n",
      " 15  daythu         517 non-null    int64  \n",
      " 16  daytue         517 non-null    int64  \n",
      " 17  daywed         517 non-null    int64  \n",
      " 18  monthapr       517 non-null    int64  \n",
      " 19  monthaug       517 non-null    int64  \n",
      " 20  monthdec       517 non-null    int64  \n",
      " 21  monthfeb       517 non-null    int64  \n",
      " 22  monthjan       517 non-null    int64  \n",
      " 23  monthjul       517 non-null    int64  \n",
      " 24  monthjun       517 non-null    int64  \n",
      " 25  monthmar       517 non-null    int64  \n",
      " 26  monthmay       517 non-null    int64  \n",
      " 27  monthnov       517 non-null    int64  \n",
      " 28  monthoct       517 non-null    int64  \n",
      " 29  monthsep       517 non-null    int64  \n",
      " 30  size_category  517 non-null    object \n",
      "dtypes: float64(8), int64(20), object(3)\n",
      "memory usage: 125.3+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 927
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1641640749393,
     "user": {
      "displayName": "Mohd Muhaz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00454428618184869141"
     },
     "user_tz": -330
    },
    "id": "w2Jqa8zZ76wi",
    "outputId": "50544295-319a-4f79-e63c-7c920709b31b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-c837322f-5e39-4d5b-8b66-0eeba80f5b7f\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FFMC</th>\n",
       "      <td>517.0</td>\n",
       "      <td>90.644681</td>\n",
       "      <td>5.520111</td>\n",
       "      <td>18.7</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.60</td>\n",
       "      <td>92.90</td>\n",
       "      <td>96.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DMC</th>\n",
       "      <td>517.0</td>\n",
       "      <td>110.872340</td>\n",
       "      <td>64.046482</td>\n",
       "      <td>1.1</td>\n",
       "      <td>68.6</td>\n",
       "      <td>108.30</td>\n",
       "      <td>142.40</td>\n",
       "      <td>291.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DC</th>\n",
       "      <td>517.0</td>\n",
       "      <td>547.940039</td>\n",
       "      <td>248.066192</td>\n",
       "      <td>7.9</td>\n",
       "      <td>437.7</td>\n",
       "      <td>664.20</td>\n",
       "      <td>713.90</td>\n",
       "      <td>860.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISI</th>\n",
       "      <td>517.0</td>\n",
       "      <td>9.021663</td>\n",
       "      <td>4.559477</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>8.40</td>\n",
       "      <td>10.80</td>\n",
       "      <td>56.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>temp</th>\n",
       "      <td>517.0</td>\n",
       "      <td>18.889168</td>\n",
       "      <td>5.806625</td>\n",
       "      <td>2.2</td>\n",
       "      <td>15.5</td>\n",
       "      <td>19.30</td>\n",
       "      <td>22.80</td>\n",
       "      <td>33.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RH</th>\n",
       "      <td>517.0</td>\n",
       "      <td>44.288201</td>\n",
       "      <td>16.317469</td>\n",
       "      <td>15.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>42.00</td>\n",
       "      <td>53.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wind</th>\n",
       "      <td>517.0</td>\n",
       "      <td>4.017602</td>\n",
       "      <td>1.791653</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.90</td>\n",
       "      <td>9.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rain</th>\n",
       "      <td>517.0</td>\n",
       "      <td>0.021663</td>\n",
       "      <td>0.295959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area</th>\n",
       "      <td>517.0</td>\n",
       "      <td>12.847292</td>\n",
       "      <td>63.655818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.52</td>\n",
       "      <td>6.57</td>\n",
       "      <td>1090.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dayfri</th>\n",
       "      <td>517.0</td>\n",
       "      <td>0.164410</td>\n",
       "      <td>0.371006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daymon</th>\n",
       "      <td>517.0</td>\n",
       "      <td>0.143133</td>\n",
       "      <td>0.350548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daysat</th>\n",
       "      <td>517.0</td>\n",
       "      <td>0.162476</td>\n",
       "      <td>0.369244</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daysun</th>\n",
       "      <td>517.0</td>\n",
       "      <td>0.183752</td>\n",
       "      <td>0.387657</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daythu</th>\n",
       "      <td>517.0</td>\n",
       "      <td>0.117988</td>\n",
       "      <td>0.322907</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daytue</th>\n",
       "      <td>517.0</td>\n",
       "      <td>0.123791</td>\n",
       "      <td>0.329662</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daywed</th>\n",
       "      <td>517.0</td>\n",
       "      <td>0.104449</td>\n",
       "      <td>0.306138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthapr</th>\n",
       "      <td>517.0</td>\n",
       "      <td>0.017408</td>\n",
       "      <td>0.130913</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthaug</th>\n",
       "      <td>517.0</td>\n",
       "      <td>0.355899</td>\n",
       "      <td>0.479249</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthdec</th>\n",
       "      <td>517.0</td>\n",
       "      <td>0.017408</td>\n",
       "      <td>0.130913</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthfeb</th>\n",
       "      <td>517.0</td>\n",
       "      <td>0.038685</td>\n",
       "      <td>0.193029</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthjan</th>\n",
       "      <td>517.0</td>\n",
       "      <td>0.003868</td>\n",
       "      <td>0.062137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthjul</th>\n",
       "      <td>517.0</td>\n",
       "      <td>0.061896</td>\n",
       "      <td>0.241199</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthjun</th>\n",
       "      <td>517.0</td>\n",
       "      <td>0.032882</td>\n",
       "      <td>0.178500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthmar</th>\n",
       "      <td>517.0</td>\n",
       "      <td>0.104449</td>\n",
       "      <td>0.306138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthmay</th>\n",
       "      <td>517.0</td>\n",
       "      <td>0.003868</td>\n",
       "      <td>0.062137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthnov</th>\n",
       "      <td>517.0</td>\n",
       "      <td>0.001934</td>\n",
       "      <td>0.043980</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthoct</th>\n",
       "      <td>517.0</td>\n",
       "      <td>0.029014</td>\n",
       "      <td>0.168007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthsep</th>\n",
       "      <td>517.0</td>\n",
       "      <td>0.332689</td>\n",
       "      <td>0.471632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c837322f-5e39-4d5b-8b66-0eeba80f5b7f')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-c837322f-5e39-4d5b-8b66-0eeba80f5b7f button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-c837322f-5e39-4d5b-8b66-0eeba80f5b7f');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "          count        mean         std   min    25%     50%     75%      max\n",
       "FFMC      517.0   90.644681    5.520111  18.7   90.2   91.60   92.90    96.20\n",
       "DMC       517.0  110.872340   64.046482   1.1   68.6  108.30  142.40   291.30\n",
       "DC        517.0  547.940039  248.066192   7.9  437.7  664.20  713.90   860.60\n",
       "ISI       517.0    9.021663    4.559477   0.0    6.5    8.40   10.80    56.10\n",
       "temp      517.0   18.889168    5.806625   2.2   15.5   19.30   22.80    33.30\n",
       "RH        517.0   44.288201   16.317469  15.0   33.0   42.00   53.00   100.00\n",
       "wind      517.0    4.017602    1.791653   0.4    2.7    4.00    4.90     9.40\n",
       "rain      517.0    0.021663    0.295959   0.0    0.0    0.00    0.00     6.40\n",
       "area      517.0   12.847292   63.655818   0.0    0.0    0.52    6.57  1090.84\n",
       "dayfri    517.0    0.164410    0.371006   0.0    0.0    0.00    0.00     1.00\n",
       "daymon    517.0    0.143133    0.350548   0.0    0.0    0.00    0.00     1.00\n",
       "daysat    517.0    0.162476    0.369244   0.0    0.0    0.00    0.00     1.00\n",
       "daysun    517.0    0.183752    0.387657   0.0    0.0    0.00    0.00     1.00\n",
       "daythu    517.0    0.117988    0.322907   0.0    0.0    0.00    0.00     1.00\n",
       "daytue    517.0    0.123791    0.329662   0.0    0.0    0.00    0.00     1.00\n",
       "daywed    517.0    0.104449    0.306138   0.0    0.0    0.00    0.00     1.00\n",
       "monthapr  517.0    0.017408    0.130913   0.0    0.0    0.00    0.00     1.00\n",
       "monthaug  517.0    0.355899    0.479249   0.0    0.0    0.00    1.00     1.00\n",
       "monthdec  517.0    0.017408    0.130913   0.0    0.0    0.00    0.00     1.00\n",
       "monthfeb  517.0    0.038685    0.193029   0.0    0.0    0.00    0.00     1.00\n",
       "monthjan  517.0    0.003868    0.062137   0.0    0.0    0.00    0.00     1.00\n",
       "monthjul  517.0    0.061896    0.241199   0.0    0.0    0.00    0.00     1.00\n",
       "monthjun  517.0    0.032882    0.178500   0.0    0.0    0.00    0.00     1.00\n",
       "monthmar  517.0    0.104449    0.306138   0.0    0.0    0.00    0.00     1.00\n",
       "monthmay  517.0    0.003868    0.062137   0.0    0.0    0.00    0.00     1.00\n",
       "monthnov  517.0    0.001934    0.043980   0.0    0.0    0.00    0.00     1.00\n",
       "monthoct  517.0    0.029014    0.168007   0.0    0.0    0.00    0.00     1.00\n",
       "monthsep  517.0    0.332689    0.471632   0.0    0.0    0.00    1.00     1.00"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 991
    },
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1641640752665,
     "user": {
      "displayName": "Mohd Muhaz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00454428618184869141"
     },
     "user_tz": -330
    },
    "id": "xuaeQFh-767Q",
    "outputId": "fb3bdc9d-f652-459c-9b36-d5659680bdc4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-413372bd-900f-4633-bd8b-5f8bac509896\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FFMC</th>\n",
       "      <th>DMC</th>\n",
       "      <th>DC</th>\n",
       "      <th>ISI</th>\n",
       "      <th>temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>wind</th>\n",
       "      <th>rain</th>\n",
       "      <th>area</th>\n",
       "      <th>dayfri</th>\n",
       "      <th>daymon</th>\n",
       "      <th>daysat</th>\n",
       "      <th>daysun</th>\n",
       "      <th>daythu</th>\n",
       "      <th>daytue</th>\n",
       "      <th>daywed</th>\n",
       "      <th>monthapr</th>\n",
       "      <th>monthaug</th>\n",
       "      <th>monthdec</th>\n",
       "      <th>monthfeb</th>\n",
       "      <th>monthjan</th>\n",
       "      <th>monthjul</th>\n",
       "      <th>monthjun</th>\n",
       "      <th>monthmar</th>\n",
       "      <th>monthmay</th>\n",
       "      <th>monthnov</th>\n",
       "      <th>monthoct</th>\n",
       "      <th>monthsep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FFMC</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.330512</td>\n",
       "      <td>0.531805</td>\n",
       "      <td>0.431532</td>\n",
       "      <td>-0.300995</td>\n",
       "      <td>-0.028485</td>\n",
       "      <td>0.056702</td>\n",
       "      <td>0.040122</td>\n",
       "      <td>0.019306</td>\n",
       "      <td>-0.059396</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.089517</td>\n",
       "      <td>0.071730</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>0.093908</td>\n",
       "      <td>-0.117199</td>\n",
       "      <td>0.228103</td>\n",
       "      <td>-0.137044</td>\n",
       "      <td>-0.281535</td>\n",
       "      <td>-0.454771</td>\n",
       "      <td>0.031833</td>\n",
       "      <td>-0.040634</td>\n",
       "      <td>-0.074327</td>\n",
       "      <td>-0.037230</td>\n",
       "      <td>-0.088964</td>\n",
       "      <td>-0.005998</td>\n",
       "      <td>0.076609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DMC</th>\n",
       "      <td>0.382619</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.682192</td>\n",
       "      <td>0.305128</td>\n",
       "      <td>0.469594</td>\n",
       "      <td>0.073795</td>\n",
       "      <td>-0.105342</td>\n",
       "      <td>0.074790</td>\n",
       "      <td>0.072994</td>\n",
       "      <td>-0.012010</td>\n",
       "      <td>-0.107921</td>\n",
       "      <td>-0.003653</td>\n",
       "      <td>0.025355</td>\n",
       "      <td>0.087672</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.017939</td>\n",
       "      <td>-0.197543</td>\n",
       "      <td>0.497928</td>\n",
       "      <td>-0.176301</td>\n",
       "      <td>-0.317899</td>\n",
       "      <td>-0.105647</td>\n",
       "      <td>-0.001946</td>\n",
       "      <td>-0.050403</td>\n",
       "      <td>-0.407404</td>\n",
       "      <td>-0.081980</td>\n",
       "      <td>-0.074218</td>\n",
       "      <td>-0.187632</td>\n",
       "      <td>0.110907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DC</th>\n",
       "      <td>0.330512</td>\n",
       "      <td>0.682192</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.229154</td>\n",
       "      <td>0.496208</td>\n",
       "      <td>-0.039192</td>\n",
       "      <td>-0.203466</td>\n",
       "      <td>0.035861</td>\n",
       "      <td>0.049383</td>\n",
       "      <td>-0.004220</td>\n",
       "      <td>-0.052993</td>\n",
       "      <td>-0.035189</td>\n",
       "      <td>-0.001431</td>\n",
       "      <td>0.051859</td>\n",
       "      <td>0.028368</td>\n",
       "      <td>0.024803</td>\n",
       "      <td>-0.268211</td>\n",
       "      <td>0.279361</td>\n",
       "      <td>-0.105642</td>\n",
       "      <td>-0.399277</td>\n",
       "      <td>-0.115064</td>\n",
       "      <td>-0.100887</td>\n",
       "      <td>-0.186183</td>\n",
       "      <td>-0.650427</td>\n",
       "      <td>-0.114209</td>\n",
       "      <td>-0.078380</td>\n",
       "      <td>0.093279</td>\n",
       "      <td>0.531857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISI</th>\n",
       "      <td>0.531805</td>\n",
       "      <td>0.305128</td>\n",
       "      <td>0.229154</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.394287</td>\n",
       "      <td>-0.132517</td>\n",
       "      <td>0.106826</td>\n",
       "      <td>0.067668</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>0.046695</td>\n",
       "      <td>-0.158601</td>\n",
       "      <td>-0.038585</td>\n",
       "      <td>-0.003243</td>\n",
       "      <td>-0.022406</td>\n",
       "      <td>0.068610</td>\n",
       "      <td>0.125415</td>\n",
       "      <td>-0.106478</td>\n",
       "      <td>0.334639</td>\n",
       "      <td>-0.162322</td>\n",
       "      <td>-0.249777</td>\n",
       "      <td>-0.103588</td>\n",
       "      <td>0.020982</td>\n",
       "      <td>0.111516</td>\n",
       "      <td>-0.143520</td>\n",
       "      <td>-0.060493</td>\n",
       "      <td>-0.076559</td>\n",
       "      <td>-0.071154</td>\n",
       "      <td>-0.068877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>temp</th>\n",
       "      <td>0.431532</td>\n",
       "      <td>0.469594</td>\n",
       "      <td>0.496208</td>\n",
       "      <td>0.394287</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.527390</td>\n",
       "      <td>-0.227116</td>\n",
       "      <td>0.069491</td>\n",
       "      <td>0.097844</td>\n",
       "      <td>-0.071949</td>\n",
       "      <td>-0.136529</td>\n",
       "      <td>0.034899</td>\n",
       "      <td>0.014403</td>\n",
       "      <td>0.051432</td>\n",
       "      <td>0.035630</td>\n",
       "      <td>0.090580</td>\n",
       "      <td>-0.157051</td>\n",
       "      <td>0.351404</td>\n",
       "      <td>-0.329648</td>\n",
       "      <td>-0.320015</td>\n",
       "      <td>-0.146520</td>\n",
       "      <td>0.142588</td>\n",
       "      <td>0.051015</td>\n",
       "      <td>-0.341797</td>\n",
       "      <td>-0.045540</td>\n",
       "      <td>-0.053798</td>\n",
       "      <td>-0.053513</td>\n",
       "      <td>0.088006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RH</th>\n",
       "      <td>-0.300995</td>\n",
       "      <td>0.073795</td>\n",
       "      <td>-0.039192</td>\n",
       "      <td>-0.132517</td>\n",
       "      <td>-0.527390</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.069410</td>\n",
       "      <td>0.099751</td>\n",
       "      <td>-0.075519</td>\n",
       "      <td>0.064506</td>\n",
       "      <td>0.009376</td>\n",
       "      <td>-0.023869</td>\n",
       "      <td>0.136220</td>\n",
       "      <td>-0.123061</td>\n",
       "      <td>-0.014211</td>\n",
       "      <td>-0.087508</td>\n",
       "      <td>0.021235</td>\n",
       "      <td>0.054761</td>\n",
       "      <td>-0.047714</td>\n",
       "      <td>0.140430</td>\n",
       "      <td>0.170923</td>\n",
       "      <td>0.013185</td>\n",
       "      <td>0.009382</td>\n",
       "      <td>-0.089836</td>\n",
       "      <td>0.086822</td>\n",
       "      <td>-0.035885</td>\n",
       "      <td>-0.072334</td>\n",
       "      <td>-0.062596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wind</th>\n",
       "      <td>-0.028485</td>\n",
       "      <td>-0.105342</td>\n",
       "      <td>-0.203466</td>\n",
       "      <td>0.106826</td>\n",
       "      <td>-0.227116</td>\n",
       "      <td>0.069410</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.061119</td>\n",
       "      <td>0.012317</td>\n",
       "      <td>0.118090</td>\n",
       "      <td>-0.063881</td>\n",
       "      <td>-0.063799</td>\n",
       "      <td>0.027981</td>\n",
       "      <td>-0.062553</td>\n",
       "      <td>0.053396</td>\n",
       "      <td>-0.019965</td>\n",
       "      <td>0.048266</td>\n",
       "      <td>0.028577</td>\n",
       "      <td>0.269702</td>\n",
       "      <td>-0.029431</td>\n",
       "      <td>-0.070245</td>\n",
       "      <td>-0.040645</td>\n",
       "      <td>0.012124</td>\n",
       "      <td>0.181433</td>\n",
       "      <td>0.015054</td>\n",
       "      <td>0.011864</td>\n",
       "      <td>-0.053850</td>\n",
       "      <td>-0.181476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rain</th>\n",
       "      <td>0.056702</td>\n",
       "      <td>0.074790</td>\n",
       "      <td>0.035861</td>\n",
       "      <td>0.067668</td>\n",
       "      <td>0.069491</td>\n",
       "      <td>0.099751</td>\n",
       "      <td>0.061119</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.007366</td>\n",
       "      <td>-0.004261</td>\n",
       "      <td>-0.029945</td>\n",
       "      <td>-0.032271</td>\n",
       "      <td>-0.017872</td>\n",
       "      <td>-0.026798</td>\n",
       "      <td>0.139311</td>\n",
       "      <td>-0.020744</td>\n",
       "      <td>-0.009752</td>\n",
       "      <td>0.093101</td>\n",
       "      <td>-0.009752</td>\n",
       "      <td>-0.014698</td>\n",
       "      <td>-0.004566</td>\n",
       "      <td>-0.013390</td>\n",
       "      <td>-0.013510</td>\n",
       "      <td>-0.020744</td>\n",
       "      <td>-0.004566</td>\n",
       "      <td>-0.003225</td>\n",
       "      <td>-0.012665</td>\n",
       "      <td>-0.051733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area</th>\n",
       "      <td>0.040122</td>\n",
       "      <td>0.072994</td>\n",
       "      <td>0.049383</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>0.097844</td>\n",
       "      <td>-0.075519</td>\n",
       "      <td>0.012317</td>\n",
       "      <td>-0.007366</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.052911</td>\n",
       "      <td>-0.021206</td>\n",
       "      <td>0.087868</td>\n",
       "      <td>-0.020463</td>\n",
       "      <td>0.020121</td>\n",
       "      <td>-0.001333</td>\n",
       "      <td>-0.011452</td>\n",
       "      <td>-0.008280</td>\n",
       "      <td>-0.004187</td>\n",
       "      <td>0.001010</td>\n",
       "      <td>-0.020732</td>\n",
       "      <td>-0.012589</td>\n",
       "      <td>0.006149</td>\n",
       "      <td>-0.020314</td>\n",
       "      <td>-0.045596</td>\n",
       "      <td>0.006264</td>\n",
       "      <td>-0.008893</td>\n",
       "      <td>-0.016878</td>\n",
       "      <td>0.056573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dayfri</th>\n",
       "      <td>0.019306</td>\n",
       "      <td>-0.012010</td>\n",
       "      <td>-0.004220</td>\n",
       "      <td>0.046695</td>\n",
       "      <td>-0.071949</td>\n",
       "      <td>0.064506</td>\n",
       "      <td>0.118090</td>\n",
       "      <td>-0.004261</td>\n",
       "      <td>-0.052911</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.181293</td>\n",
       "      <td>-0.195372</td>\n",
       "      <td>-0.210462</td>\n",
       "      <td>-0.162237</td>\n",
       "      <td>-0.166728</td>\n",
       "      <td>-0.151487</td>\n",
       "      <td>-0.019140</td>\n",
       "      <td>-0.100837</td>\n",
       "      <td>-0.019140</td>\n",
       "      <td>0.046323</td>\n",
       "      <td>-0.027643</td>\n",
       "      <td>-0.048969</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.036205</td>\n",
       "      <td>0.056423</td>\n",
       "      <td>-0.019527</td>\n",
       "      <td>-0.045585</td>\n",
       "      <td>0.107671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daymon</th>\n",
       "      <td>-0.059396</td>\n",
       "      <td>-0.107921</td>\n",
       "      <td>-0.052993</td>\n",
       "      <td>-0.158601</td>\n",
       "      <td>-0.136529</td>\n",
       "      <td>0.009376</td>\n",
       "      <td>-0.063881</td>\n",
       "      <td>-0.029945</td>\n",
       "      <td>-0.021206</td>\n",
       "      <td>-0.181293</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.180015</td>\n",
       "      <td>-0.193919</td>\n",
       "      <td>-0.149485</td>\n",
       "      <td>-0.153623</td>\n",
       "      <td>-0.139579</td>\n",
       "      <td>-0.012171</td>\n",
       "      <td>-0.130774</td>\n",
       "      <td>0.114519</td>\n",
       "      <td>0.003933</td>\n",
       "      <td>-0.025470</td>\n",
       "      <td>-0.013300</td>\n",
       "      <td>0.017553</td>\n",
       "      <td>0.077125</td>\n",
       "      <td>-0.025470</td>\n",
       "      <td>-0.017992</td>\n",
       "      <td>0.060975</td>\n",
       "      <td>0.039632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daysat</th>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.003653</td>\n",
       "      <td>-0.035189</td>\n",
       "      <td>-0.038585</td>\n",
       "      <td>0.034899</td>\n",
       "      <td>-0.023869</td>\n",
       "      <td>-0.063799</td>\n",
       "      <td>-0.032271</td>\n",
       "      <td>0.087868</td>\n",
       "      <td>-0.195372</td>\n",
       "      <td>-0.180015</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.208978</td>\n",
       "      <td>-0.161094</td>\n",
       "      <td>-0.165553</td>\n",
       "      <td>-0.150419</td>\n",
       "      <td>-0.018534</td>\n",
       "      <td>-0.009808</td>\n",
       "      <td>-0.058625</td>\n",
       "      <td>0.020406</td>\n",
       "      <td>0.057019</td>\n",
       "      <td>0.060945</td>\n",
       "      <td>-0.022408</td>\n",
       "      <td>0.021024</td>\n",
       "      <td>0.057019</td>\n",
       "      <td>-0.019390</td>\n",
       "      <td>0.017584</td>\n",
       "      <td>-0.032783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daysun</th>\n",
       "      <td>-0.089517</td>\n",
       "      <td>0.025355</td>\n",
       "      <td>-0.001431</td>\n",
       "      <td>-0.003243</td>\n",
       "      <td>0.014403</td>\n",
       "      <td>0.136220</td>\n",
       "      <td>0.027981</td>\n",
       "      <td>-0.017872</td>\n",
       "      <td>-0.020463</td>\n",
       "      <td>-0.210462</td>\n",
       "      <td>-0.193919</td>\n",
       "      <td>-0.208978</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.173535</td>\n",
       "      <td>-0.178339</td>\n",
       "      <td>-0.162036</td>\n",
       "      <td>0.051409</td>\n",
       "      <td>0.064566</td>\n",
       "      <td>-0.024966</td>\n",
       "      <td>0.008416</td>\n",
       "      <td>0.050887</td>\n",
       "      <td>-0.018241</td>\n",
       "      <td>0.024540</td>\n",
       "      <td>-0.047726</td>\n",
       "      <td>-0.029568</td>\n",
       "      <td>-0.020887</td>\n",
       "      <td>0.007252</td>\n",
       "      <td>-0.048817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daythu</th>\n",
       "      <td>0.071730</td>\n",
       "      <td>0.087672</td>\n",
       "      <td>0.051859</td>\n",
       "      <td>-0.022406</td>\n",
       "      <td>0.051432</td>\n",
       "      <td>-0.123061</td>\n",
       "      <td>-0.062553</td>\n",
       "      <td>-0.026798</td>\n",
       "      <td>0.020121</td>\n",
       "      <td>-0.162237</td>\n",
       "      <td>-0.149485</td>\n",
       "      <td>-0.161094</td>\n",
       "      <td>-0.173535</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.137475</td>\n",
       "      <td>-0.124908</td>\n",
       "      <td>0.043007</td>\n",
       "      <td>0.053726</td>\n",
       "      <td>-0.002838</td>\n",
       "      <td>-0.042278</td>\n",
       "      <td>-0.022793</td>\n",
       "      <td>-0.019300</td>\n",
       "      <td>-0.000195</td>\n",
       "      <td>-0.026885</td>\n",
       "      <td>-0.022793</td>\n",
       "      <td>-0.016101</td>\n",
       "      <td>-0.063223</td>\n",
       "      <td>0.008984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daytue</th>\n",
       "      <td>0.011225</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.028368</td>\n",
       "      <td>0.068610</td>\n",
       "      <td>0.035630</td>\n",
       "      <td>-0.014211</td>\n",
       "      <td>0.053396</td>\n",
       "      <td>0.139311</td>\n",
       "      <td>-0.001333</td>\n",
       "      <td>-0.166728</td>\n",
       "      <td>-0.153623</td>\n",
       "      <td>-0.165553</td>\n",
       "      <td>-0.178339</td>\n",
       "      <td>-0.137475</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.128365</td>\n",
       "      <td>-0.050030</td>\n",
       "      <td>0.064061</td>\n",
       "      <td>-0.005125</td>\n",
       "      <td>-0.014491</td>\n",
       "      <td>-0.023424</td>\n",
       "      <td>0.049688</td>\n",
       "      <td>-0.069308</td>\n",
       "      <td>-0.032351</td>\n",
       "      <td>-0.023424</td>\n",
       "      <td>0.117121</td>\n",
       "      <td>0.005008</td>\n",
       "      <td>-0.028570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daywed</th>\n",
       "      <td>0.093908</td>\n",
       "      <td>0.017939</td>\n",
       "      <td>0.024803</td>\n",
       "      <td>0.125415</td>\n",
       "      <td>0.090580</td>\n",
       "      <td>-0.087508</td>\n",
       "      <td>-0.019965</td>\n",
       "      <td>-0.020744</td>\n",
       "      <td>-0.011452</td>\n",
       "      <td>-0.151487</td>\n",
       "      <td>-0.139579</td>\n",
       "      <td>-0.150419</td>\n",
       "      <td>-0.162036</td>\n",
       "      <td>-0.124908</td>\n",
       "      <td>-0.128365</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002899</td>\n",
       "      <td>0.076367</td>\n",
       "      <td>0.002899</td>\n",
       "      <td>-0.035713</td>\n",
       "      <td>-0.021282</td>\n",
       "      <td>-0.008985</td>\n",
       "      <td>0.043422</td>\n",
       "      <td>-0.033917</td>\n",
       "      <td>-0.021282</td>\n",
       "      <td>-0.015034</td>\n",
       "      <td>0.016325</td>\n",
       "      <td>-0.053222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthapr</th>\n",
       "      <td>-0.117199</td>\n",
       "      <td>-0.197543</td>\n",
       "      <td>-0.268211</td>\n",
       "      <td>-0.106478</td>\n",
       "      <td>-0.157051</td>\n",
       "      <td>0.021235</td>\n",
       "      <td>0.048266</td>\n",
       "      <td>-0.009752</td>\n",
       "      <td>-0.008280</td>\n",
       "      <td>-0.019140</td>\n",
       "      <td>-0.012171</td>\n",
       "      <td>-0.018534</td>\n",
       "      <td>0.051409</td>\n",
       "      <td>0.043007</td>\n",
       "      <td>-0.050030</td>\n",
       "      <td>0.002899</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.098941</td>\n",
       "      <td>-0.017717</td>\n",
       "      <td>-0.026701</td>\n",
       "      <td>-0.008295</td>\n",
       "      <td>-0.034190</td>\n",
       "      <td>-0.024543</td>\n",
       "      <td>-0.045456</td>\n",
       "      <td>-0.008295</td>\n",
       "      <td>-0.005860</td>\n",
       "      <td>-0.023008</td>\n",
       "      <td>-0.093982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthaug</th>\n",
       "      <td>0.228103</td>\n",
       "      <td>0.497928</td>\n",
       "      <td>0.279361</td>\n",
       "      <td>0.334639</td>\n",
       "      <td>0.351404</td>\n",
       "      <td>0.054761</td>\n",
       "      <td>0.028577</td>\n",
       "      <td>0.093101</td>\n",
       "      <td>-0.004187</td>\n",
       "      <td>-0.100837</td>\n",
       "      <td>-0.130774</td>\n",
       "      <td>-0.009808</td>\n",
       "      <td>0.064566</td>\n",
       "      <td>0.053726</td>\n",
       "      <td>0.064061</td>\n",
       "      <td>0.076367</td>\n",
       "      <td>-0.098941</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.098941</td>\n",
       "      <td>-0.149116</td>\n",
       "      <td>-0.046323</td>\n",
       "      <td>-0.190937</td>\n",
       "      <td>-0.137065</td>\n",
       "      <td>-0.253859</td>\n",
       "      <td>-0.046323</td>\n",
       "      <td>-0.032724</td>\n",
       "      <td>-0.128493</td>\n",
       "      <td>-0.524858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthdec</th>\n",
       "      <td>-0.137044</td>\n",
       "      <td>-0.176301</td>\n",
       "      <td>-0.105642</td>\n",
       "      <td>-0.162322</td>\n",
       "      <td>-0.329648</td>\n",
       "      <td>-0.047714</td>\n",
       "      <td>0.269702</td>\n",
       "      <td>-0.009752</td>\n",
       "      <td>0.001010</td>\n",
       "      <td>-0.019140</td>\n",
       "      <td>0.114519</td>\n",
       "      <td>-0.058625</td>\n",
       "      <td>-0.024966</td>\n",
       "      <td>-0.002838</td>\n",
       "      <td>-0.005125</td>\n",
       "      <td>0.002899</td>\n",
       "      <td>-0.017717</td>\n",
       "      <td>-0.098941</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.026701</td>\n",
       "      <td>-0.008295</td>\n",
       "      <td>-0.034190</td>\n",
       "      <td>-0.024543</td>\n",
       "      <td>-0.045456</td>\n",
       "      <td>-0.008295</td>\n",
       "      <td>-0.005860</td>\n",
       "      <td>-0.023008</td>\n",
       "      <td>-0.093982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthfeb</th>\n",
       "      <td>-0.281535</td>\n",
       "      <td>-0.317899</td>\n",
       "      <td>-0.399277</td>\n",
       "      <td>-0.249777</td>\n",
       "      <td>-0.320015</td>\n",
       "      <td>0.140430</td>\n",
       "      <td>-0.029431</td>\n",
       "      <td>-0.014698</td>\n",
       "      <td>-0.020732</td>\n",
       "      <td>0.046323</td>\n",
       "      <td>0.003933</td>\n",
       "      <td>0.020406</td>\n",
       "      <td>0.008416</td>\n",
       "      <td>-0.042278</td>\n",
       "      <td>-0.014491</td>\n",
       "      <td>-0.035713</td>\n",
       "      <td>-0.026701</td>\n",
       "      <td>-0.149116</td>\n",
       "      <td>-0.026701</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.012501</td>\n",
       "      <td>-0.051528</td>\n",
       "      <td>-0.036989</td>\n",
       "      <td>-0.068508</td>\n",
       "      <td>-0.012501</td>\n",
       "      <td>-0.008831</td>\n",
       "      <td>-0.034676</td>\n",
       "      <td>-0.141642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthjan</th>\n",
       "      <td>-0.454771</td>\n",
       "      <td>-0.105647</td>\n",
       "      <td>-0.115064</td>\n",
       "      <td>-0.103588</td>\n",
       "      <td>-0.146520</td>\n",
       "      <td>0.170923</td>\n",
       "      <td>-0.070245</td>\n",
       "      <td>-0.004566</td>\n",
       "      <td>-0.012589</td>\n",
       "      <td>-0.027643</td>\n",
       "      <td>-0.025470</td>\n",
       "      <td>0.057019</td>\n",
       "      <td>0.050887</td>\n",
       "      <td>-0.022793</td>\n",
       "      <td>-0.023424</td>\n",
       "      <td>-0.021282</td>\n",
       "      <td>-0.008295</td>\n",
       "      <td>-0.046323</td>\n",
       "      <td>-0.008295</td>\n",
       "      <td>-0.012501</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.016007</td>\n",
       "      <td>-0.011491</td>\n",
       "      <td>-0.021282</td>\n",
       "      <td>-0.003883</td>\n",
       "      <td>-0.002743</td>\n",
       "      <td>-0.010772</td>\n",
       "      <td>-0.044001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthjul</th>\n",
       "      <td>0.031833</td>\n",
       "      <td>-0.001946</td>\n",
       "      <td>-0.100887</td>\n",
       "      <td>0.020982</td>\n",
       "      <td>0.142588</td>\n",
       "      <td>0.013185</td>\n",
       "      <td>-0.040645</td>\n",
       "      <td>-0.013390</td>\n",
       "      <td>0.006149</td>\n",
       "      <td>-0.048969</td>\n",
       "      <td>-0.013300</td>\n",
       "      <td>0.060945</td>\n",
       "      <td>-0.018241</td>\n",
       "      <td>-0.019300</td>\n",
       "      <td>0.049688</td>\n",
       "      <td>-0.008985</td>\n",
       "      <td>-0.034190</td>\n",
       "      <td>-0.190937</td>\n",
       "      <td>-0.034190</td>\n",
       "      <td>-0.051528</td>\n",
       "      <td>-0.016007</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.047363</td>\n",
       "      <td>-0.087722</td>\n",
       "      <td>-0.016007</td>\n",
       "      <td>-0.011308</td>\n",
       "      <td>-0.044402</td>\n",
       "      <td>-0.181367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthjun</th>\n",
       "      <td>-0.040634</td>\n",
       "      <td>-0.050403</td>\n",
       "      <td>-0.186183</td>\n",
       "      <td>0.111516</td>\n",
       "      <td>0.051015</td>\n",
       "      <td>0.009382</td>\n",
       "      <td>0.012124</td>\n",
       "      <td>-0.013510</td>\n",
       "      <td>-0.020314</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.017553</td>\n",
       "      <td>-0.022408</td>\n",
       "      <td>0.024540</td>\n",
       "      <td>-0.000195</td>\n",
       "      <td>-0.069308</td>\n",
       "      <td>0.043422</td>\n",
       "      <td>-0.024543</td>\n",
       "      <td>-0.137065</td>\n",
       "      <td>-0.024543</td>\n",
       "      <td>-0.036989</td>\n",
       "      <td>-0.011491</td>\n",
       "      <td>-0.047363</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.062972</td>\n",
       "      <td>-0.011491</td>\n",
       "      <td>-0.008117</td>\n",
       "      <td>-0.031874</td>\n",
       "      <td>-0.130195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthmar</th>\n",
       "      <td>-0.074327</td>\n",
       "      <td>-0.407404</td>\n",
       "      <td>-0.650427</td>\n",
       "      <td>-0.143520</td>\n",
       "      <td>-0.341797</td>\n",
       "      <td>-0.089836</td>\n",
       "      <td>0.181433</td>\n",
       "      <td>-0.020744</td>\n",
       "      <td>-0.045596</td>\n",
       "      <td>0.036205</td>\n",
       "      <td>0.077125</td>\n",
       "      <td>0.021024</td>\n",
       "      <td>-0.047726</td>\n",
       "      <td>-0.026885</td>\n",
       "      <td>-0.032351</td>\n",
       "      <td>-0.033917</td>\n",
       "      <td>-0.045456</td>\n",
       "      <td>-0.253859</td>\n",
       "      <td>-0.045456</td>\n",
       "      <td>-0.068508</td>\n",
       "      <td>-0.021282</td>\n",
       "      <td>-0.087722</td>\n",
       "      <td>-0.062972</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.021282</td>\n",
       "      <td>-0.015034</td>\n",
       "      <td>-0.059034</td>\n",
       "      <td>-0.241135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthmay</th>\n",
       "      <td>-0.037230</td>\n",
       "      <td>-0.081980</td>\n",
       "      <td>-0.114209</td>\n",
       "      <td>-0.060493</td>\n",
       "      <td>-0.045540</td>\n",
       "      <td>0.086822</td>\n",
       "      <td>0.015054</td>\n",
       "      <td>-0.004566</td>\n",
       "      <td>0.006264</td>\n",
       "      <td>0.056423</td>\n",
       "      <td>-0.025470</td>\n",
       "      <td>0.057019</td>\n",
       "      <td>-0.029568</td>\n",
       "      <td>-0.022793</td>\n",
       "      <td>-0.023424</td>\n",
       "      <td>-0.021282</td>\n",
       "      <td>-0.008295</td>\n",
       "      <td>-0.046323</td>\n",
       "      <td>-0.008295</td>\n",
       "      <td>-0.012501</td>\n",
       "      <td>-0.003883</td>\n",
       "      <td>-0.016007</td>\n",
       "      <td>-0.011491</td>\n",
       "      <td>-0.021282</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.002743</td>\n",
       "      <td>-0.010772</td>\n",
       "      <td>-0.044001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthnov</th>\n",
       "      <td>-0.088964</td>\n",
       "      <td>-0.074218</td>\n",
       "      <td>-0.078380</td>\n",
       "      <td>-0.076559</td>\n",
       "      <td>-0.053798</td>\n",
       "      <td>-0.035885</td>\n",
       "      <td>0.011864</td>\n",
       "      <td>-0.003225</td>\n",
       "      <td>-0.008893</td>\n",
       "      <td>-0.019527</td>\n",
       "      <td>-0.017992</td>\n",
       "      <td>-0.019390</td>\n",
       "      <td>-0.020887</td>\n",
       "      <td>-0.016101</td>\n",
       "      <td>0.117121</td>\n",
       "      <td>-0.015034</td>\n",
       "      <td>-0.005860</td>\n",
       "      <td>-0.032724</td>\n",
       "      <td>-0.005860</td>\n",
       "      <td>-0.008831</td>\n",
       "      <td>-0.002743</td>\n",
       "      <td>-0.011308</td>\n",
       "      <td>-0.008117</td>\n",
       "      <td>-0.015034</td>\n",
       "      <td>-0.002743</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.007610</td>\n",
       "      <td>-0.031083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthoct</th>\n",
       "      <td>-0.005998</td>\n",
       "      <td>-0.187632</td>\n",
       "      <td>0.093279</td>\n",
       "      <td>-0.071154</td>\n",
       "      <td>-0.053513</td>\n",
       "      <td>-0.072334</td>\n",
       "      <td>-0.053850</td>\n",
       "      <td>-0.012665</td>\n",
       "      <td>-0.016878</td>\n",
       "      <td>-0.045585</td>\n",
       "      <td>0.060975</td>\n",
       "      <td>0.017584</td>\n",
       "      <td>0.007252</td>\n",
       "      <td>-0.063223</td>\n",
       "      <td>0.005008</td>\n",
       "      <td>0.016325</td>\n",
       "      <td>-0.023008</td>\n",
       "      <td>-0.128493</td>\n",
       "      <td>-0.023008</td>\n",
       "      <td>-0.034676</td>\n",
       "      <td>-0.010772</td>\n",
       "      <td>-0.044402</td>\n",
       "      <td>-0.031874</td>\n",
       "      <td>-0.059034</td>\n",
       "      <td>-0.010772</td>\n",
       "      <td>-0.007610</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.122053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthsep</th>\n",
       "      <td>0.076609</td>\n",
       "      <td>0.110907</td>\n",
       "      <td>0.531857</td>\n",
       "      <td>-0.068877</td>\n",
       "      <td>0.088006</td>\n",
       "      <td>-0.062596</td>\n",
       "      <td>-0.181476</td>\n",
       "      <td>-0.051733</td>\n",
       "      <td>0.056573</td>\n",
       "      <td>0.107671</td>\n",
       "      <td>0.039632</td>\n",
       "      <td>-0.032783</td>\n",
       "      <td>-0.048817</td>\n",
       "      <td>0.008984</td>\n",
       "      <td>-0.028570</td>\n",
       "      <td>-0.053222</td>\n",
       "      <td>-0.093982</td>\n",
       "      <td>-0.524858</td>\n",
       "      <td>-0.093982</td>\n",
       "      <td>-0.141642</td>\n",
       "      <td>-0.044001</td>\n",
       "      <td>-0.181367</td>\n",
       "      <td>-0.130195</td>\n",
       "      <td>-0.241135</td>\n",
       "      <td>-0.044001</td>\n",
       "      <td>-0.031083</td>\n",
       "      <td>-0.122053</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-413372bd-900f-4633-bd8b-5f8bac509896')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-413372bd-900f-4633-bd8b-5f8bac509896 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-413372bd-900f-4633-bd8b-5f8bac509896');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "              FFMC       DMC        DC  ...  monthnov  monthoct  monthsep\n",
       "FFMC      1.000000  0.382619  0.330512  ... -0.088964 -0.005998  0.076609\n",
       "DMC       0.382619  1.000000  0.682192  ... -0.074218 -0.187632  0.110907\n",
       "DC        0.330512  0.682192  1.000000  ... -0.078380  0.093279  0.531857\n",
       "ISI       0.531805  0.305128  0.229154  ... -0.076559 -0.071154 -0.068877\n",
       "temp      0.431532  0.469594  0.496208  ... -0.053798 -0.053513  0.088006\n",
       "RH       -0.300995  0.073795 -0.039192  ... -0.035885 -0.072334 -0.062596\n",
       "wind     -0.028485 -0.105342 -0.203466  ...  0.011864 -0.053850 -0.181476\n",
       "rain      0.056702  0.074790  0.035861  ... -0.003225 -0.012665 -0.051733\n",
       "area      0.040122  0.072994  0.049383  ... -0.008893 -0.016878  0.056573\n",
       "dayfri    0.019306 -0.012010 -0.004220  ... -0.019527 -0.045585  0.107671\n",
       "daymon   -0.059396 -0.107921 -0.052993  ... -0.017992  0.060975  0.039632\n",
       "daysat   -0.019637 -0.003653 -0.035189  ... -0.019390  0.017584 -0.032783\n",
       "daysun   -0.089517  0.025355 -0.001431  ... -0.020887  0.007252 -0.048817\n",
       "daythu    0.071730  0.087672  0.051859  ... -0.016101 -0.063223  0.008984\n",
       "daytue    0.011225  0.000016  0.028368  ...  0.117121  0.005008 -0.028570\n",
       "daywed    0.093908  0.017939  0.024803  ... -0.015034  0.016325 -0.053222\n",
       "monthapr -0.117199 -0.197543 -0.268211  ... -0.005860 -0.023008 -0.093982\n",
       "monthaug  0.228103  0.497928  0.279361  ... -0.032724 -0.128493 -0.524858\n",
       "monthdec -0.137044 -0.176301 -0.105642  ... -0.005860 -0.023008 -0.093982\n",
       "monthfeb -0.281535 -0.317899 -0.399277  ... -0.008831 -0.034676 -0.141642\n",
       "monthjan -0.454771 -0.105647 -0.115064  ... -0.002743 -0.010772 -0.044001\n",
       "monthjul  0.031833 -0.001946 -0.100887  ... -0.011308 -0.044402 -0.181367\n",
       "monthjun -0.040634 -0.050403 -0.186183  ... -0.008117 -0.031874 -0.130195\n",
       "monthmar -0.074327 -0.407404 -0.650427  ... -0.015034 -0.059034 -0.241135\n",
       "monthmay -0.037230 -0.081980 -0.114209  ... -0.002743 -0.010772 -0.044001\n",
       "monthnov -0.088964 -0.074218 -0.078380  ...  1.000000 -0.007610 -0.031083\n",
       "monthoct -0.005998 -0.187632  0.093279  ... -0.007610  1.000000 -0.122053\n",
       "monthsep  0.076609  0.110907  0.531857  ... -0.031083 -0.122053  1.000000\n",
       "\n",
       "[28 rows x 28 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1641640755300,
     "user": {
      "displayName": "Mohd Muhaz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00454428618184869141"
     },
     "user_tz": -330
    },
    "id": "VEtGCfZ-972p"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "data['size_category']= label_encoder.fit_transform(data['size_category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1641640757604,
     "user": {
      "displayName": "Mohd Muhaz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00454428618184869141"
     },
     "user_tz": -330
    },
    "id": "AxTqHnP0AstS",
    "outputId": "ca0164b1-1995-4a2c-822f-decc126f7bf8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-ad7edd74-cca2-4dbd-b04d-d60cb826a5ea\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FFMC</th>\n",
       "      <th>DMC</th>\n",
       "      <th>DC</th>\n",
       "      <th>ISI</th>\n",
       "      <th>temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>wind</th>\n",
       "      <th>rain</th>\n",
       "      <th>area</th>\n",
       "      <th>dayfri</th>\n",
       "      <th>daymon</th>\n",
       "      <th>daysat</th>\n",
       "      <th>daysun</th>\n",
       "      <th>daythu</th>\n",
       "      <th>daytue</th>\n",
       "      <th>daywed</th>\n",
       "      <th>monthapr</th>\n",
       "      <th>monthaug</th>\n",
       "      <th>monthdec</th>\n",
       "      <th>monthfeb</th>\n",
       "      <th>monthjan</th>\n",
       "      <th>monthjul</th>\n",
       "      <th>monthjun</th>\n",
       "      <th>monthmar</th>\n",
       "      <th>monthmay</th>\n",
       "      <th>monthnov</th>\n",
       "      <th>monthoct</th>\n",
       "      <th>monthsep</th>\n",
       "      <th>size_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86.2</td>\n",
       "      <td>26.2</td>\n",
       "      <td>94.3</td>\n",
       "      <td>5.1</td>\n",
       "      <td>8.2</td>\n",
       "      <td>51</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.6</td>\n",
       "      <td>35.4</td>\n",
       "      <td>669.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>18.0</td>\n",
       "      <td>33</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.6</td>\n",
       "      <td>43.7</td>\n",
       "      <td>686.9</td>\n",
       "      <td>6.7</td>\n",
       "      <td>14.6</td>\n",
       "      <td>33</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91.7</td>\n",
       "      <td>33.3</td>\n",
       "      <td>77.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>97</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89.3</td>\n",
       "      <td>51.3</td>\n",
       "      <td>102.2</td>\n",
       "      <td>9.6</td>\n",
       "      <td>11.4</td>\n",
       "      <td>99</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>27.8</td>\n",
       "      <td>32</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>21.9</td>\n",
       "      <td>71</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>21.2</td>\n",
       "      <td>70</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>94.4</td>\n",
       "      <td>146.0</td>\n",
       "      <td>614.7</td>\n",
       "      <td>11.3</td>\n",
       "      <td>25.6</td>\n",
       "      <td>42</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>79.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>106.7</td>\n",
       "      <td>1.1</td>\n",
       "      <td>11.8</td>\n",
       "      <td>31</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>517 rows Ã— 29 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ad7edd74-cca2-4dbd-b04d-d60cb826a5ea')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-ad7edd74-cca2-4dbd-b04d-d60cb826a5ea button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-ad7edd74-cca2-4dbd-b04d-d60cb826a5ea');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "     FFMC    DMC     DC   ISI  ...  monthnov  monthoct  monthsep  size_category\n",
       "0    86.2   26.2   94.3   5.1  ...         0         0         0              1\n",
       "1    90.6   35.4  669.1   6.7  ...         0         1         0              1\n",
       "2    90.6   43.7  686.9   6.7  ...         0         1         0              1\n",
       "3    91.7   33.3   77.5   9.0  ...         0         0         0              1\n",
       "4    89.3   51.3  102.2   9.6  ...         0         0         0              1\n",
       "..    ...    ...    ...   ...  ...       ...       ...       ...            ...\n",
       "512  81.6   56.7  665.6   1.9  ...         0         0         0              0\n",
       "513  81.6   56.7  665.6   1.9  ...         0         0         0              0\n",
       "514  81.6   56.7  665.6   1.9  ...         0         0         0              0\n",
       "515  94.4  146.0  614.7  11.3  ...         0         0         0              1\n",
       "516  79.5    3.0  106.7   1.1  ...         1         0         0              1\n",
       "\n",
       "[517 rows x 29 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=data.drop(columns=['month','day'], axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1641640759478,
     "user": {
      "displayName": "Mohd Muhaz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00454428618184869141"
     },
     "user_tz": -330
    },
    "id": "XEOvqVk798j1",
    "outputId": "f5fcaaec-8fec-409a-e565-761e28d105b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 517 entries, 0 to 516\n",
      "Data columns (total 29 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   FFMC           517 non-null    float64\n",
      " 1   DMC            517 non-null    float64\n",
      " 2   DC             517 non-null    float64\n",
      " 3   ISI            517 non-null    float64\n",
      " 4   temp           517 non-null    float64\n",
      " 5   RH             517 non-null    int64  \n",
      " 6   wind           517 non-null    float64\n",
      " 7   rain           517 non-null    float64\n",
      " 8   area           517 non-null    float64\n",
      " 9   dayfri         517 non-null    int64  \n",
      " 10  daymon         517 non-null    int64  \n",
      " 11  daysat         517 non-null    int64  \n",
      " 12  daysun         517 non-null    int64  \n",
      " 13  daythu         517 non-null    int64  \n",
      " 14  daytue         517 non-null    int64  \n",
      " 15  daywed         517 non-null    int64  \n",
      " 16  monthapr       517 non-null    int64  \n",
      " 17  monthaug       517 non-null    int64  \n",
      " 18  monthdec       517 non-null    int64  \n",
      " 19  monthfeb       517 non-null    int64  \n",
      " 20  monthjan       517 non-null    int64  \n",
      " 21  monthjul       517 non-null    int64  \n",
      " 22  monthjun       517 non-null    int64  \n",
      " 23  monthmar       517 non-null    int64  \n",
      " 24  monthmay       517 non-null    int64  \n",
      " 25  monthnov       517 non-null    int64  \n",
      " 26  monthoct       517 non-null    int64  \n",
      " 27  monthsep       517 non-null    int64  \n",
      " 28  size_category  517 non-null    int64  \n",
      "dtypes: float64(8), int64(21)\n",
      "memory usage: 117.3 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1641640760900,
     "user": {
      "displayName": "Mohd Muhaz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00454428618184869141"
     },
     "user_tz": -330
    },
    "id": "SM1tbud698qV"
   },
   "outputs": [],
   "source": [
    "x = data.iloc[:,0:-1]\n",
    "y = data['size_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1641640763086,
     "user": {
      "displayName": "Mohd Muhaz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00454428618184869141"
     },
     "user_tz": -330
    },
    "id": "e36iKCtF98vt",
    "outputId": "c0ab054c-2cca-4317-98e7-b4acf5c2bda4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-6965f4ea-f4bd-4b3a-9e20-86be82b2a61c\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FFMC</th>\n",
       "      <th>DMC</th>\n",
       "      <th>DC</th>\n",
       "      <th>ISI</th>\n",
       "      <th>temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>wind</th>\n",
       "      <th>rain</th>\n",
       "      <th>area</th>\n",
       "      <th>dayfri</th>\n",
       "      <th>daymon</th>\n",
       "      <th>daysat</th>\n",
       "      <th>daysun</th>\n",
       "      <th>daythu</th>\n",
       "      <th>daytue</th>\n",
       "      <th>daywed</th>\n",
       "      <th>monthapr</th>\n",
       "      <th>monthaug</th>\n",
       "      <th>monthdec</th>\n",
       "      <th>monthfeb</th>\n",
       "      <th>monthjan</th>\n",
       "      <th>monthjul</th>\n",
       "      <th>monthjun</th>\n",
       "      <th>monthmar</th>\n",
       "      <th>monthmay</th>\n",
       "      <th>monthnov</th>\n",
       "      <th>monthoct</th>\n",
       "      <th>monthsep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86.2</td>\n",
       "      <td>26.2</td>\n",
       "      <td>94.3</td>\n",
       "      <td>5.1</td>\n",
       "      <td>8.2</td>\n",
       "      <td>51</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.6</td>\n",
       "      <td>35.4</td>\n",
       "      <td>669.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>18.0</td>\n",
       "      <td>33</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.6</td>\n",
       "      <td>43.7</td>\n",
       "      <td>686.9</td>\n",
       "      <td>6.7</td>\n",
       "      <td>14.6</td>\n",
       "      <td>33</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91.7</td>\n",
       "      <td>33.3</td>\n",
       "      <td>77.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>97</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89.3</td>\n",
       "      <td>51.3</td>\n",
       "      <td>102.2</td>\n",
       "      <td>9.6</td>\n",
       "      <td>11.4</td>\n",
       "      <td>99</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>27.8</td>\n",
       "      <td>32</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>21.9</td>\n",
       "      <td>71</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>21.2</td>\n",
       "      <td>70</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>94.4</td>\n",
       "      <td>146.0</td>\n",
       "      <td>614.7</td>\n",
       "      <td>11.3</td>\n",
       "      <td>25.6</td>\n",
       "      <td>42</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>79.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>106.7</td>\n",
       "      <td>1.1</td>\n",
       "      <td>11.8</td>\n",
       "      <td>31</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>517 rows Ã— 28 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6965f4ea-f4bd-4b3a-9e20-86be82b2a61c')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-6965f4ea-f4bd-4b3a-9e20-86be82b2a61c button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-6965f4ea-f4bd-4b3a-9e20-86be82b2a61c');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "     FFMC    DMC     DC   ISI  ...  monthmay  monthnov  monthoct  monthsep\n",
       "0    86.2   26.2   94.3   5.1  ...         0         0         0         0\n",
       "1    90.6   35.4  669.1   6.7  ...         0         0         1         0\n",
       "2    90.6   43.7  686.9   6.7  ...         0         0         1         0\n",
       "3    91.7   33.3   77.5   9.0  ...         0         0         0         0\n",
       "4    89.3   51.3  102.2   9.6  ...         0         0         0         0\n",
       "..    ...    ...    ...   ...  ...       ...       ...       ...       ...\n",
       "512  81.6   56.7  665.6   1.9  ...         0         0         0         0\n",
       "513  81.6   56.7  665.6   1.9  ...         0         0         0         0\n",
       "514  81.6   56.7  665.6   1.9  ...         0         0         0         0\n",
       "515  94.4  146.0  614.7  11.3  ...         0         0         0         0\n",
       "516  79.5    3.0  106.7   1.1  ...         0         1         0         0\n",
       "\n",
       "[517 rows x 28 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1641640764691,
     "user": {
      "displayName": "Mohd Muhaz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00454428618184869141"
     },
     "user_tz": -330
    },
    "id": "oNaN9t4Z980o",
    "outputId": "32e39b43-affc-4df5-9109-4961092f82e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "512    0\n",
       "513    0\n",
       "514    0\n",
       "515    1\n",
       "516    1\n",
       "Name: size_category, Length: 517, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1641640766456,
     "user": {
      "displayName": "Mohd Muhaz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00454428618184869141"
     },
     "user_tz": -330
    },
    "id": "uxzRqgpz989_"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIYmbSs5SkjU"
   },
   "source": [
    "#**Artificial Neural Network Model - Backpropagation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1641640769796,
     "user": {
      "displayName": "Mohd Muhaz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00454428618184869141"
     },
     "user_tz": -330
    },
    "id": "B6LCReI099CU"
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(42, input_dim=28, activation='relu'))\n",
    "model.add(Dense(28, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1641640771447,
     "user": {
      "displayName": "Mohd Muhaz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00454428618184869141"
     },
     "user_tz": -330
    },
    "id": "9WnSwCgI99I-"
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21627,
     "status": "ok",
     "timestamp": 1641640807870,
     "user": {
      "displayName": "Mohd Muhaz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00454428618184869141"
     },
     "user_tz": -330
    },
    "id": "b3RoCa5K99L2",
    "outputId": "92890056-f99f-4a79-cf22-75883477def0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/180\n",
      "28/28 [==============================] - 1s 8ms/step - loss: 2.4334 - accuracy: 0.6739 - val_loss: 1.0354 - val_accuracy: 0.7956\n",
      "Epoch 2/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5931 - accuracy: 0.8442 - val_loss: 0.3018 - val_accuracy: 0.8686\n",
      "Epoch 3/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.2278 - accuracy: 0.9130 - val_loss: 0.1998 - val_accuracy: 0.9562\n",
      "Epoch 4/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3863 - accuracy: 0.8587 - val_loss: 0.1789 - val_accuracy: 0.9635\n",
      "Epoch 5/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.2034 - accuracy: 0.9094 - val_loss: 0.2149 - val_accuracy: 0.8905\n",
      "Epoch 6/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3724 - accuracy: 0.8841 - val_loss: 0.2595 - val_accuracy: 0.8832\n",
      "Epoch 7/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1693 - accuracy: 0.9348 - val_loss: 0.2246 - val_accuracy: 0.8905\n",
      "Epoch 8/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1419 - accuracy: 0.9457 - val_loss: 0.3996 - val_accuracy: 0.8613\n",
      "Epoch 9/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1404 - accuracy: 0.9493 - val_loss: 0.2110 - val_accuracy: 0.8978\n",
      "Epoch 10/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.1522 - accuracy: 0.9420 - val_loss: 0.1856 - val_accuracy: 0.9635\n",
      "Epoch 11/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.1291 - accuracy: 0.9493 - val_loss: 0.1990 - val_accuracy: 0.9635\n",
      "Epoch 12/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1797 - accuracy: 0.9203 - val_loss: 0.5708 - val_accuracy: 0.8686\n",
      "Epoch 13/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.2173 - accuracy: 0.9384 - val_loss: 0.1293 - val_accuracy: 0.9343\n",
      "Epoch 14/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.2096 - accuracy: 0.9203 - val_loss: 0.1447 - val_accuracy: 0.9124\n",
      "Epoch 15/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.2676 - accuracy: 0.9130 - val_loss: 0.1761 - val_accuracy: 0.9051\n",
      "Epoch 16/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1018 - accuracy: 0.9674 - val_loss: 0.1179 - val_accuracy: 0.9562\n",
      "Epoch 17/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0975 - accuracy: 0.9674 - val_loss: 0.1105 - val_accuracy: 0.9635\n",
      "Epoch 18/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1567 - accuracy: 0.9493 - val_loss: 0.3002 - val_accuracy: 0.8832\n",
      "Epoch 19/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.1415 - accuracy: 0.9420 - val_loss: 0.1124 - val_accuracy: 0.9343\n",
      "Epoch 20/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1356 - accuracy: 0.9457 - val_loss: 0.2261 - val_accuracy: 0.8832\n",
      "Epoch 21/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0979 - accuracy: 0.9638 - val_loss: 0.1012 - val_accuracy: 0.9781\n",
      "Epoch 22/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0788 - accuracy: 0.9819 - val_loss: 0.1046 - val_accuracy: 0.9781\n",
      "Epoch 23/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0739 - accuracy: 0.9783 - val_loss: 0.1202 - val_accuracy: 0.9270\n",
      "Epoch 24/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0720 - accuracy: 0.9746 - val_loss: 0.0957 - val_accuracy: 0.9781\n",
      "Epoch 25/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0724 - accuracy: 0.9710 - val_loss: 0.2955 - val_accuracy: 0.8905\n",
      "Epoch 26/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1710 - accuracy: 0.9384 - val_loss: 0.0865 - val_accuracy: 0.9781\n",
      "Epoch 27/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0648 - accuracy: 0.9855 - val_loss: 0.1695 - val_accuracy: 0.9197\n",
      "Epoch 28/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1869 - accuracy: 0.9348 - val_loss: 0.3189 - val_accuracy: 0.8832\n",
      "Epoch 29/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.1668 - accuracy: 0.9275 - val_loss: 0.0853 - val_accuracy: 0.9562\n",
      "Epoch 30/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.1070 - accuracy: 0.9601 - val_loss: 0.1993 - val_accuracy: 0.9124\n",
      "Epoch 31/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.2116 - accuracy: 0.9312 - val_loss: 0.1367 - val_accuracy: 0.9562\n",
      "Epoch 32/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0573 - accuracy: 0.9819 - val_loss: 0.0796 - val_accuracy: 0.9781\n",
      "Epoch 33/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0842 - accuracy: 0.9746 - val_loss: 0.0777 - val_accuracy: 0.9781\n",
      "Epoch 34/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0449 - accuracy: 0.9928 - val_loss: 0.0828 - val_accuracy: 0.9854\n",
      "Epoch 35/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.1318 - accuracy: 0.9529 - val_loss: 0.2435 - val_accuracy: 0.8978\n",
      "Epoch 36/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.2910 - accuracy: 0.9130 - val_loss: 0.2104 - val_accuracy: 0.9124\n",
      "Epoch 37/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0553 - accuracy: 0.9819 - val_loss: 0.0820 - val_accuracy: 0.9781\n",
      "Epoch 38/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1163 - accuracy: 0.9529 - val_loss: 0.2893 - val_accuracy: 0.8905\n",
      "Epoch 39/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0578 - accuracy: 0.9819 - val_loss: 0.0761 - val_accuracy: 0.9781\n",
      "Epoch 40/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0453 - accuracy: 0.9928 - val_loss: 0.0896 - val_accuracy: 0.9489\n",
      "Epoch 41/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0885 - accuracy: 0.9783 - val_loss: 0.0928 - val_accuracy: 0.9708\n",
      "Epoch 42/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0562 - accuracy: 0.9783 - val_loss: 0.1115 - val_accuracy: 0.9562\n",
      "Epoch 43/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0595 - accuracy: 0.9783 - val_loss: 0.0820 - val_accuracy: 0.9708\n",
      "Epoch 44/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0602 - accuracy: 0.9783 - val_loss: 0.0716 - val_accuracy: 0.9781\n",
      "Epoch 45/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0785 - accuracy: 0.9746 - val_loss: 0.1565 - val_accuracy: 0.9343\n",
      "Epoch 46/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.1057 - accuracy: 0.9638 - val_loss: 0.0756 - val_accuracy: 0.9635\n",
      "Epoch 47/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0363 - accuracy: 0.9928 - val_loss: 0.2541 - val_accuracy: 0.9124\n",
      "Epoch 48/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0744 - accuracy: 0.9783 - val_loss: 0.1350 - val_accuracy: 0.9197\n",
      "Epoch 49/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0411 - accuracy: 0.9964 - val_loss: 0.0856 - val_accuracy: 0.9489\n",
      "Epoch 50/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0663 - accuracy: 0.9855 - val_loss: 0.1713 - val_accuracy: 0.9197\n",
      "Epoch 51/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0422 - accuracy: 0.9928 - val_loss: 0.1956 - val_accuracy: 0.9197\n",
      "Epoch 52/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0999 - accuracy: 0.9710 - val_loss: 0.1837 - val_accuracy: 0.9197\n",
      "Epoch 53/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0756 - accuracy: 0.9710 - val_loss: 0.0937 - val_accuracy: 0.9489\n",
      "Epoch 54/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0462 - accuracy: 0.9819 - val_loss: 0.0713 - val_accuracy: 0.9781\n",
      "Epoch 55/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0595 - accuracy: 0.9783 - val_loss: 0.1024 - val_accuracy: 0.9343\n",
      "Epoch 56/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0581 - accuracy: 0.9710 - val_loss: 0.0672 - val_accuracy: 0.9781\n",
      "Epoch 57/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0473 - accuracy: 0.9855 - val_loss: 0.1625 - val_accuracy: 0.9343\n",
      "Epoch 58/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0387 - accuracy: 0.9855 - val_loss: 0.0635 - val_accuracy: 0.9708\n",
      "Epoch 59/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0351 - accuracy: 0.9855 - val_loss: 0.0915 - val_accuracy: 0.9562\n",
      "Epoch 60/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0518 - accuracy: 0.9746 - val_loss: 0.0807 - val_accuracy: 0.9708\n",
      "Epoch 61/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0517 - accuracy: 0.9855 - val_loss: 0.1140 - val_accuracy: 0.9562\n",
      "Epoch 62/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0500 - accuracy: 0.9819 - val_loss: 0.0744 - val_accuracy: 0.9708\n",
      "Epoch 63/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1436 - accuracy: 0.9529 - val_loss: 0.3491 - val_accuracy: 0.8832\n",
      "Epoch 64/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0811 - accuracy: 0.9710 - val_loss: 0.4089 - val_accuracy: 0.8905\n",
      "Epoch 65/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0335 - accuracy: 0.9928 - val_loss: 0.1561 - val_accuracy: 0.9343\n",
      "Epoch 66/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0875 - accuracy: 0.9674 - val_loss: 0.1339 - val_accuracy: 0.9562\n",
      "Epoch 67/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0360 - accuracy: 0.9928 - val_loss: 0.0698 - val_accuracy: 0.9708\n",
      "Epoch 68/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0302 - accuracy: 0.9928 - val_loss: 0.0874 - val_accuracy: 0.9489\n",
      "Epoch 69/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0733 - accuracy: 0.9710 - val_loss: 0.3868 - val_accuracy: 0.8978\n",
      "Epoch 70/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.2022 - accuracy: 0.9493 - val_loss: 0.0757 - val_accuracy: 0.9562\n",
      "Epoch 71/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1200 - accuracy: 0.9457 - val_loss: 0.0552 - val_accuracy: 0.9854\n",
      "Epoch 72/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.1240 - accuracy: 0.9710 - val_loss: 0.1058 - val_accuracy: 0.9708\n",
      "Epoch 73/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0304 - accuracy: 0.9928 - val_loss: 0.0563 - val_accuracy: 0.9781\n",
      "Epoch 74/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.1373 - accuracy: 0.9529 - val_loss: 0.0770 - val_accuracy: 0.9781\n",
      "Epoch 75/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0642 - accuracy: 0.9746 - val_loss: 0.2292 - val_accuracy: 0.9197\n",
      "Epoch 76/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.1483 - accuracy: 0.9638 - val_loss: 0.1819 - val_accuracy: 0.9343\n",
      "Epoch 77/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0513 - accuracy: 0.9783 - val_loss: 0.0886 - val_accuracy: 0.9562\n",
      "Epoch 78/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0673 - accuracy: 0.9783 - val_loss: 0.0795 - val_accuracy: 0.9489\n",
      "Epoch 79/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0220 - accuracy: 0.9928 - val_loss: 0.0521 - val_accuracy: 0.9854\n",
      "Epoch 80/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0255 - accuracy: 0.9855 - val_loss: 0.1223 - val_accuracy: 0.9343\n",
      "Epoch 81/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5518 - accuracy: 0.8877 - val_loss: 1.4122 - val_accuracy: 0.8686\n",
      "Epoch 82/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.1172 - accuracy: 0.9710 - val_loss: 0.1105 - val_accuracy: 0.9343\n",
      "Epoch 83/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0470 - accuracy: 0.9819 - val_loss: 0.2138 - val_accuracy: 0.9270\n",
      "Epoch 84/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0447 - accuracy: 0.9783 - val_loss: 0.0711 - val_accuracy: 0.9781\n",
      "Epoch 85/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0326 - accuracy: 0.9855 - val_loss: 0.0567 - val_accuracy: 0.9854\n",
      "Epoch 86/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0286 - accuracy: 0.9891 - val_loss: 0.0532 - val_accuracy: 0.9854\n",
      "Epoch 87/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0211 - accuracy: 0.9964 - val_loss: 0.1048 - val_accuracy: 0.9489\n",
      "Epoch 88/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.9891 - val_loss: 0.2540 - val_accuracy: 0.9270\n",
      "Epoch 89/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0535 - accuracy: 0.9746 - val_loss: 0.0829 - val_accuracy: 0.9489\n",
      "Epoch 90/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0274 - accuracy: 0.9819 - val_loss: 0.1214 - val_accuracy: 0.9416\n",
      "Epoch 91/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0303 - accuracy: 0.9891 - val_loss: 0.0906 - val_accuracy: 0.9781\n",
      "Epoch 92/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.2474 - accuracy: 0.9384 - val_loss: 0.9120 - val_accuracy: 0.8905\n",
      "Epoch 93/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1108 - accuracy: 0.9674 - val_loss: 0.0604 - val_accuracy: 0.9854\n",
      "Epoch 94/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0202 - accuracy: 0.9891 - val_loss: 0.0646 - val_accuracy: 0.9854\n",
      "Epoch 95/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0280 - accuracy: 0.9855 - val_loss: 0.0608 - val_accuracy: 0.9708\n",
      "Epoch 96/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0525 - accuracy: 0.9855 - val_loss: 0.2375 - val_accuracy: 0.9343\n",
      "Epoch 97/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.2610 - accuracy: 0.9493 - val_loss: 0.0529 - val_accuracy: 0.9781\n",
      "Epoch 98/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0847 - accuracy: 0.9710 - val_loss: 0.1546 - val_accuracy: 0.9562\n",
      "Epoch 99/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.2394 - accuracy: 0.9384 - val_loss: 0.0834 - val_accuracy: 0.9489\n",
      "Epoch 100/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0636 - accuracy: 0.9710 - val_loss: 0.7370 - val_accuracy: 0.8905\n",
      "Epoch 101/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.1553 - accuracy: 0.9529 - val_loss: 0.0590 - val_accuracy: 0.9854\n",
      "Epoch 102/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0305 - accuracy: 0.9891 - val_loss: 0.0739 - val_accuracy: 0.9562\n",
      "Epoch 103/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0386 - accuracy: 0.9855 - val_loss: 0.1017 - val_accuracy: 0.9489\n",
      "Epoch 104/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0143 - accuracy: 1.0000 - val_loss: 0.0945 - val_accuracy: 0.9489\n",
      "Epoch 105/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0536 - accuracy: 0.9891 - val_loss: 0.0596 - val_accuracy: 0.9854\n",
      "Epoch 106/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0532 - accuracy: 0.9783 - val_loss: 0.0682 - val_accuracy: 0.9854\n",
      "Epoch 107/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0126 - accuracy: 0.9964 - val_loss: 0.1337 - val_accuracy: 0.9343\n",
      "Epoch 108/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0435 - accuracy: 0.9783 - val_loss: 0.1195 - val_accuracy: 0.9635\n",
      "Epoch 109/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0593 - accuracy: 0.9746 - val_loss: 0.3424 - val_accuracy: 0.9197\n",
      "Epoch 110/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1013 - accuracy: 0.9674 - val_loss: 0.2941 - val_accuracy: 0.9197\n",
      "Epoch 111/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0630 - accuracy: 0.9746 - val_loss: 0.0738 - val_accuracy: 0.9781\n",
      "Epoch 112/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0836 - accuracy: 0.9746 - val_loss: 0.1423 - val_accuracy: 0.9343\n",
      "Epoch 113/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0383 - accuracy: 0.9855 - val_loss: 0.0992 - val_accuracy: 0.9489\n",
      "Epoch 114/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0181 - accuracy: 0.9891 - val_loss: 0.0443 - val_accuracy: 0.9927\n",
      "Epoch 115/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0216 - accuracy: 0.9891 - val_loss: 0.0465 - val_accuracy: 0.9927\n",
      "Epoch 116/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0169 - accuracy: 0.9964 - val_loss: 0.0416 - val_accuracy: 0.9927\n",
      "Epoch 117/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0323 - accuracy: 0.9855 - val_loss: 0.0840 - val_accuracy: 0.9489\n",
      "Epoch 118/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0159 - accuracy: 0.9928 - val_loss: 0.0489 - val_accuracy: 0.9781\n",
      "Epoch 119/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.1361 - accuracy: 0.9529 - val_loss: 0.1258 - val_accuracy: 0.9562\n",
      "Epoch 120/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0922 - accuracy: 0.9783 - val_loss: 0.5065 - val_accuracy: 0.9124\n",
      "Epoch 121/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0773 - accuracy: 0.9710 - val_loss: 0.0395 - val_accuracy: 0.9927\n",
      "Epoch 122/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0472 - accuracy: 0.9819 - val_loss: 0.0578 - val_accuracy: 0.9854\n",
      "Epoch 123/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0755 - accuracy: 0.9746 - val_loss: 0.3583 - val_accuracy: 0.9124\n",
      "Epoch 124/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0390 - accuracy: 0.9855 - val_loss: 0.0579 - val_accuracy: 0.9927\n",
      "Epoch 125/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0194 - accuracy: 0.9928 - val_loss: 0.0819 - val_accuracy: 0.9708\n",
      "Epoch 126/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0591 - accuracy: 0.9783 - val_loss: 0.0666 - val_accuracy: 0.9854\n",
      "Epoch 127/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0343 - accuracy: 0.9928 - val_loss: 0.2281 - val_accuracy: 0.9343\n",
      "Epoch 128/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1192 - accuracy: 0.9601 - val_loss: 0.1788 - val_accuracy: 0.9343\n",
      "Epoch 129/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0207 - accuracy: 0.9891 - val_loss: 0.1171 - val_accuracy: 0.9562\n",
      "Epoch 130/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0220 - accuracy: 0.9928 - val_loss: 0.0638 - val_accuracy: 0.9781\n",
      "Epoch 131/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0625 - accuracy: 0.9783 - val_loss: 0.0479 - val_accuracy: 0.9854\n",
      "Epoch 132/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0193 - accuracy: 0.9928 - val_loss: 0.0443 - val_accuracy: 0.9927\n",
      "Epoch 133/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.9928 - val_loss: 0.0741 - val_accuracy: 0.9635\n",
      "Epoch 134/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0131 - accuracy: 0.9964 - val_loss: 0.2208 - val_accuracy: 0.9343\n",
      "Epoch 135/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0557 - accuracy: 0.9819 - val_loss: 0.0906 - val_accuracy: 0.9708\n",
      "Epoch 136/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0873 - accuracy: 0.9746 - val_loss: 0.0905 - val_accuracy: 0.9489\n",
      "Epoch 137/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0277 - accuracy: 0.9891 - val_loss: 0.0471 - val_accuracy: 0.9781\n",
      "Epoch 138/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0370 - accuracy: 0.9855 - val_loss: 0.5944 - val_accuracy: 0.8978\n",
      "Epoch 139/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.2365 - accuracy: 0.9420 - val_loss: 0.3247 - val_accuracy: 0.9343\n",
      "Epoch 140/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0696 - accuracy: 0.9783 - val_loss: 0.0982 - val_accuracy: 0.9781\n",
      "Epoch 141/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.1060 - accuracy: 0.9746 - val_loss: 0.6180 - val_accuracy: 0.9051\n",
      "Epoch 142/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0848 - accuracy: 0.9783 - val_loss: 0.0623 - val_accuracy: 0.9854\n",
      "Epoch 143/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0117 - accuracy: 0.9964 - val_loss: 0.0539 - val_accuracy: 0.9854\n",
      "Epoch 144/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0207 - accuracy: 0.9891 - val_loss: 0.1934 - val_accuracy: 0.9343\n",
      "Epoch 145/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0322 - accuracy: 0.9855 - val_loss: 0.1446 - val_accuracy: 0.9562\n",
      "Epoch 146/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0366 - accuracy: 0.9891 - val_loss: 0.0539 - val_accuracy: 0.9854\n",
      "Epoch 147/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0116 - accuracy: 0.9964 - val_loss: 0.0606 - val_accuracy: 0.9854\n",
      "Epoch 148/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0424 - accuracy: 0.9855 - val_loss: 0.0985 - val_accuracy: 0.9562\n",
      "Epoch 149/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0378 - accuracy: 0.9819 - val_loss: 0.0620 - val_accuracy: 0.9781\n",
      "Epoch 150/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0198 - accuracy: 0.9928 - val_loss: 0.0700 - val_accuracy: 0.9781\n",
      "Epoch 151/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0428 - accuracy: 0.9891 - val_loss: 0.0636 - val_accuracy: 0.9854\n",
      "Epoch 152/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0130 - accuracy: 0.9891 - val_loss: 0.0557 - val_accuracy: 0.9854\n",
      "Epoch 153/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0461 - accuracy: 0.9891 - val_loss: 0.0997 - val_accuracy: 0.9562\n",
      "Epoch 154/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0112 - accuracy: 0.9928 - val_loss: 0.1325 - val_accuracy: 0.9635\n",
      "Epoch 155/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.1322 - accuracy: 0.9783 - val_loss: 0.2415 - val_accuracy: 0.9343\n",
      "Epoch 156/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0468 - accuracy: 0.9819 - val_loss: 0.2096 - val_accuracy: 0.9416\n",
      "Epoch 157/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.1186 - accuracy: 0.9601 - val_loss: 0.0620 - val_accuracy: 0.9781\n",
      "Epoch 158/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1024 - accuracy: 0.9710 - val_loss: 0.0566 - val_accuracy: 0.9781\n",
      "Epoch 159/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0438 - accuracy: 0.9855 - val_loss: 0.1437 - val_accuracy: 0.9343\n",
      "Epoch 160/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0472 - accuracy: 0.9855 - val_loss: 0.2472 - val_accuracy: 0.9343\n",
      "Epoch 161/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0217 - accuracy: 0.9928 - val_loss: 0.0659 - val_accuracy: 0.9854\n",
      "Epoch 162/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0171 - accuracy: 0.9928 - val_loss: 0.0952 - val_accuracy: 0.9489\n",
      "Epoch 163/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0676 - accuracy: 0.9746 - val_loss: 0.6052 - val_accuracy: 0.8540\n",
      "Epoch 164/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3808 - accuracy: 0.9312 - val_loss: 0.3632 - val_accuracy: 0.9416\n",
      "Epoch 165/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0850 - accuracy: 0.9746 - val_loss: 0.0389 - val_accuracy: 0.9927\n",
      "Epoch 166/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0249 - accuracy: 0.9855 - val_loss: 0.1174 - val_accuracy: 0.9489\n",
      "Epoch 167/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0682 - accuracy: 0.9746 - val_loss: 0.1198 - val_accuracy: 0.9416\n",
      "Epoch 168/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0797 - accuracy: 0.9710 - val_loss: 0.0683 - val_accuracy: 0.9708\n",
      "Epoch 169/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0334 - accuracy: 0.9891 - val_loss: 0.2790 - val_accuracy: 0.9270\n",
      "Epoch 170/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0592 - accuracy: 0.9819 - val_loss: 0.0750 - val_accuracy: 0.9635\n",
      "Epoch 171/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0573 - accuracy: 0.9746 - val_loss: 0.0641 - val_accuracy: 0.9781\n",
      "Epoch 172/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1167 - accuracy: 0.9674 - val_loss: 0.2191 - val_accuracy: 0.9416\n",
      "Epoch 173/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0354 - accuracy: 0.9855 - val_loss: 0.0564 - val_accuracy: 0.9854\n",
      "Epoch 174/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0067 - accuracy: 0.9964 - val_loss: 0.0662 - val_accuracy: 0.9854\n",
      "Epoch 175/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0152 - accuracy: 0.9891 - val_loss: 0.0529 - val_accuracy: 0.9854\n",
      "Epoch 176/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0121 - accuracy: 0.9928 - val_loss: 0.0512 - val_accuracy: 0.9927\n",
      "Epoch 177/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0444 - accuracy: 0.9855 - val_loss: 0.1624 - val_accuracy: 0.9416\n",
      "Epoch 178/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0947 - accuracy: 0.9783 - val_loss: 0.3413 - val_accuracy: 0.9343\n",
      "Epoch 179/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.1583 - accuracy: 0.9710 - val_loss: 0.2999 - val_accuracy: 0.9343\n",
      "Epoch 180/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0103 - accuracy: 0.9964 - val_loss: 0.0738 - val_accuracy: 0.9854\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history=model.fit(x_train,y_train, validation_split=0.33, epochs=180, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 483,
     "status": "ok",
     "timestamp": 1641640819009,
     "user": {
      "displayName": "Mohd Muhaz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00454428618184869141"
     },
     "user_tz": -330
    },
    "id": "d6ehrfV4e8xs",
    "outputId": "7b244ffc-8ce1-482e-a45c-eb043882c3c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 1ms/step - loss: 0.0478 - accuracy: 0.9845\n",
      "accuracy: 98.45%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(x, y)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 458,
     "status": "ok",
     "timestamp": 1641640853136,
     "user": {
      "displayName": "Mohd Muhaz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00454428618184869141"
     },
     "user_tz": -330
    },
    "id": "RQEm_lr699T6",
    "outputId": "d5c2757c-2c22-4e06-b8cb-af6297087981"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize training history\n",
    "\n",
    "# list all data in history\n",
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "executionInfo": {
     "elapsed": 1145,
     "status": "ok",
     "timestamp": 1641640858575,
     "user": {
      "displayName": "Mohd Muhaz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00454428618184869141"
     },
     "user_tz": -330
    },
    "id": "Y9EaVQgC77Bi",
    "outputId": "632e9603-ef62-48c0-a3eb-fe7dab2b1a25"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnQxGh1CE4zN"
   },
   "source": [
    "#**Hyper Parameter Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 480,
     "status": "ok",
     "timestamp": 1641641649709,
     "user": {
      "displayName": "Mohd Muhaz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00454428618184869141"
     },
     "user_tz": -330
    },
    "id": "dDqLTmzpE42T"
   },
   "outputs": [],
   "source": [
    "X = data.iloc[:,0:-1]\n",
    "Y = data['size_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 539,
     "status": "ok",
     "timestamp": 1641640948063,
     "user": {
      "displayName": "Mohd Muhaz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00454428618184869141"
     },
     "user_tz": -330
    },
    "id": "_7dTrkSegBxE"
   },
   "outputs": [],
   "source": [
    "# Standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "a = StandardScaler()\n",
    "a.fit(X)\n",
    "X_standardized = a.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 927
    },
    "executionInfo": {
     "elapsed": 544,
     "status": "ok",
     "timestamp": 1641640954888,
     "user": {
      "displayName": "Mohd Muhaz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00454428618184869141"
     },
     "user_tz": -330
    },
    "id": "7a_Jy8_aE49U",
    "outputId": "1a6f1f47-0da2-466e-de28-8c78201f27b5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-6a2dbe3d-51c9-4011-8dda-a9fb71be49ba\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>517.0</td>\n",
       "      <td>-1.754024e-15</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-13.045818</td>\n",
       "      <td>-0.080635</td>\n",
       "      <td>0.173229</td>\n",
       "      <td>0.408960</td>\n",
       "      <td>1.007353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>517.0</td>\n",
       "      <td>3.070830e-16</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-1.715608</td>\n",
       "      <td>-0.660665</td>\n",
       "      <td>-0.040203</td>\n",
       "      <td>0.492739</td>\n",
       "      <td>2.819865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>517.0</td>\n",
       "      <td>7.387171e-17</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-2.179108</td>\n",
       "      <td>-0.444828</td>\n",
       "      <td>0.469119</td>\n",
       "      <td>0.669663</td>\n",
       "      <td>1.261610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>517.0</td>\n",
       "      <td>-3.865380e-17</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-1.980578</td>\n",
       "      <td>-0.553595</td>\n",
       "      <td>-0.136477</td>\n",
       "      <td>0.390409</td>\n",
       "      <td>10.335381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>517.0</td>\n",
       "      <td>2.005703e-16</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-2.876943</td>\n",
       "      <td>-0.584238</td>\n",
       "      <td>0.070821</td>\n",
       "      <td>0.674164</td>\n",
       "      <td>2.484195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>517.0</td>\n",
       "      <td>3.362881e-16</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-1.796637</td>\n",
       "      <td>-0.692456</td>\n",
       "      <td>-0.140366</td>\n",
       "      <td>0.534411</td>\n",
       "      <td>3.417549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>517.0</td>\n",
       "      <td>-2.676776e-16</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-2.021098</td>\n",
       "      <td>-0.736124</td>\n",
       "      <td>-0.009834</td>\n",
       "      <td>0.492982</td>\n",
       "      <td>3.007063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>517.0</td>\n",
       "      <td>-2.841054e-16</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-0.073268</td>\n",
       "      <td>-0.073268</td>\n",
       "      <td>-0.073268</td>\n",
       "      <td>-0.073268</td>\n",
       "      <td>21.572284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>517.0</td>\n",
       "      <td>-1.274502e-16</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-0.202020</td>\n",
       "      <td>-0.202020</td>\n",
       "      <td>-0.193843</td>\n",
       "      <td>-0.098709</td>\n",
       "      <td>16.951110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>517.0</td>\n",
       "      <td>4.874674e-17</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-0.443576</td>\n",
       "      <td>-0.443576</td>\n",
       "      <td>-0.443576</td>\n",
       "      <td>-0.443576</td>\n",
       "      <td>2.254407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>517.0</td>\n",
       "      <td>-1.868267e-16</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-0.408709</td>\n",
       "      <td>-0.408709</td>\n",
       "      <td>-0.408709</td>\n",
       "      <td>-0.408709</td>\n",
       "      <td>2.446730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>517.0</td>\n",
       "      <td>-2.238699e-16</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-0.440449</td>\n",
       "      <td>-0.440449</td>\n",
       "      <td>-0.440449</td>\n",
       "      <td>-0.440449</td>\n",
       "      <td>2.270410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>517.0</td>\n",
       "      <td>-6.098711e-17</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-0.474467</td>\n",
       "      <td>-0.474467</td>\n",
       "      <td>-0.474467</td>\n",
       "      <td>-0.474467</td>\n",
       "      <td>2.107630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>517.0</td>\n",
       "      <td>-1.004999e-16</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-0.365748</td>\n",
       "      <td>-0.365748</td>\n",
       "      <td>-0.365748</td>\n",
       "      <td>-0.365748</td>\n",
       "      <td>2.734120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>517.0</td>\n",
       "      <td>2.405125e-17</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-0.375873</td>\n",
       "      <td>-0.375873</td>\n",
       "      <td>-0.375873</td>\n",
       "      <td>-0.375873</td>\n",
       "      <td>2.660475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>517.0</td>\n",
       "      <td>-3.843906e-17</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-0.341512</td>\n",
       "      <td>-0.341512</td>\n",
       "      <td>-0.341512</td>\n",
       "      <td>-0.341512</td>\n",
       "      <td>2.928152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>517.0</td>\n",
       "      <td>-1.344293e-16</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-0.133103</td>\n",
       "      <td>-0.133103</td>\n",
       "      <td>-0.133103</td>\n",
       "      <td>-0.133103</td>\n",
       "      <td>7.512952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>517.0</td>\n",
       "      <td>2.473843e-16</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-0.743339</td>\n",
       "      <td>-0.743339</td>\n",
       "      <td>-0.743339</td>\n",
       "      <td>1.345282</td>\n",
       "      <td>1.345282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>517.0</td>\n",
       "      <td>7.179943e-16</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-0.133103</td>\n",
       "      <td>-0.133103</td>\n",
       "      <td>-0.133103</td>\n",
       "      <td>-0.133103</td>\n",
       "      <td>7.512952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>517.0</td>\n",
       "      <td>-1.933764e-16</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-0.200603</td>\n",
       "      <td>-0.200603</td>\n",
       "      <td>-0.200603</td>\n",
       "      <td>-0.200603</td>\n",
       "      <td>4.984977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>517.0</td>\n",
       "      <td>-2.260174e-17</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-0.062318</td>\n",
       "      <td>-0.062318</td>\n",
       "      <td>-0.062318</td>\n",
       "      <td>-0.062318</td>\n",
       "      <td>16.046807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>517.0</td>\n",
       "      <td>1.352883e-17</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-0.256865</td>\n",
       "      <td>-0.256865</td>\n",
       "      <td>-0.256865</td>\n",
       "      <td>-0.256865</td>\n",
       "      <td>3.893103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>517.0</td>\n",
       "      <td>1.169277e-16</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-0.184391</td>\n",
       "      <td>-0.184391</td>\n",
       "      <td>-0.184391</td>\n",
       "      <td>-0.184391</td>\n",
       "      <td>5.423261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>517.0</td>\n",
       "      <td>2.265542e-16</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-0.341512</td>\n",
       "      <td>-0.341512</td>\n",
       "      <td>-0.341512</td>\n",
       "      <td>-0.341512</td>\n",
       "      <td>2.928152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>517.0</td>\n",
       "      <td>-2.596515e-16</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-0.062318</td>\n",
       "      <td>-0.062318</td>\n",
       "      <td>-0.062318</td>\n",
       "      <td>-0.062318</td>\n",
       "      <td>16.046807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>517.0</td>\n",
       "      <td>1.443075e-16</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-0.044023</td>\n",
       "      <td>-0.044023</td>\n",
       "      <td>-0.044023</td>\n",
       "      <td>-0.044023</td>\n",
       "      <td>22.715633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>517.0</td>\n",
       "      <td>6.253326e-16</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-0.172860</td>\n",
       "      <td>-0.172860</td>\n",
       "      <td>-0.172860</td>\n",
       "      <td>-0.172860</td>\n",
       "      <td>5.785038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>517.0</td>\n",
       "      <td>4.024290e-16</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-0.706081</td>\n",
       "      <td>-0.706081</td>\n",
       "      <td>-0.706081</td>\n",
       "      <td>1.416268</td>\n",
       "      <td>1.416268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6a2dbe3d-51c9-4011-8dda-a9fb71be49ba')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-6a2dbe3d-51c9-4011-8dda-a9fb71be49ba button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-6a2dbe3d-51c9-4011-8dda-a9fb71be49ba');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "    count          mean       std  ...       50%       75%        max\n",
       "0   517.0 -1.754024e-15  1.000969  ...  0.173229  0.408960   1.007353\n",
       "1   517.0  3.070830e-16  1.000969  ... -0.040203  0.492739   2.819865\n",
       "2   517.0  7.387171e-17  1.000969  ...  0.469119  0.669663   1.261610\n",
       "3   517.0 -3.865380e-17  1.000969  ... -0.136477  0.390409  10.335381\n",
       "4   517.0  2.005703e-16  1.000969  ...  0.070821  0.674164   2.484195\n",
       "5   517.0  3.362881e-16  1.000969  ... -0.140366  0.534411   3.417549\n",
       "6   517.0 -2.676776e-16  1.000969  ... -0.009834  0.492982   3.007063\n",
       "7   517.0 -2.841054e-16  1.000969  ... -0.073268 -0.073268  21.572284\n",
       "8   517.0 -1.274502e-16  1.000969  ... -0.193843 -0.098709  16.951110\n",
       "9   517.0  4.874674e-17  1.000969  ... -0.443576 -0.443576   2.254407\n",
       "10  517.0 -1.868267e-16  1.000969  ... -0.408709 -0.408709   2.446730\n",
       "11  517.0 -2.238699e-16  1.000969  ... -0.440449 -0.440449   2.270410\n",
       "12  517.0 -6.098711e-17  1.000969  ... -0.474467 -0.474467   2.107630\n",
       "13  517.0 -1.004999e-16  1.000969  ... -0.365748 -0.365748   2.734120\n",
       "14  517.0  2.405125e-17  1.000969  ... -0.375873 -0.375873   2.660475\n",
       "15  517.0 -3.843906e-17  1.000969  ... -0.341512 -0.341512   2.928152\n",
       "16  517.0 -1.344293e-16  1.000969  ... -0.133103 -0.133103   7.512952\n",
       "17  517.0  2.473843e-16  1.000969  ... -0.743339  1.345282   1.345282\n",
       "18  517.0  7.179943e-16  1.000969  ... -0.133103 -0.133103   7.512952\n",
       "19  517.0 -1.933764e-16  1.000969  ... -0.200603 -0.200603   4.984977\n",
       "20  517.0 -2.260174e-17  1.000969  ... -0.062318 -0.062318  16.046807\n",
       "21  517.0  1.352883e-17  1.000969  ... -0.256865 -0.256865   3.893103\n",
       "22  517.0  1.169277e-16  1.000969  ... -0.184391 -0.184391   5.423261\n",
       "23  517.0  2.265542e-16  1.000969  ... -0.341512 -0.341512   2.928152\n",
       "24  517.0 -2.596515e-16  1.000969  ... -0.062318 -0.062318  16.046807\n",
       "25  517.0  1.443075e-16  1.000969  ... -0.044023 -0.044023  22.715633\n",
       "26  517.0  6.253326e-16  1.000969  ... -0.172860 -0.172860   5.785038\n",
       "27  517.0  4.024290e-16  1.000969  ... -0.706081  1.416268   1.416268\n",
       "\n",
       "[28 rows x 8 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_standardized).describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnSZHoWTE5Cs"
   },
   "source": [
    "#**Tuning of Hyperparameters :- Batch Size and Epochs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 562,
     "status": "ok",
     "timestamp": 1641642675583,
     "user": {
      "displayName": "Mohd Muhaz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00454428618184869141"
     },
     "user_tz": -330
    },
    "id": "nyoG2YhfE5FG"
   },
   "outputs": [],
   "source": [
    "# Importing the necessary packages\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1641642677350,
     "user": {
      "displayName": "Mohd Muhaz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00454428618184869141"
     },
     "user_tz": -330
    },
    "id": "mX9NOpFGE5Hh"
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=28, kernel_initializer='uniform', activation='relu'))\n",
    "    model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "    \n",
    "    adam=Adam(lr=0.01)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 90787,
     "status": "ok",
     "timestamp": 1641642769797,
     "user": {
      "displayName": "Mohd Muhaz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00454428618184869141"
     },
     "user_tz": -330
    },
    "id": "dGTJjR50E5KW",
    "outputId": "ef1bbd9d-69ac-4ccf-e5d0-59726689e73a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
      "  \n",
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "[CV 1/5; 1/9] START batch_size=10, epochs=10....................................\n",
      "[CV 1/5; 1/9] END .....batch_size=10, epochs=10;, score=1.000 total time=   1.2s\n",
      "[CV 2/5; 1/9] START batch_size=10, epochs=10....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/9] END .....batch_size=10, epochs=10;, score=0.808 total time=   1.0s\n",
      "[CV 3/5; 1/9] START batch_size=10, epochs=10....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/9] END .....batch_size=10, epochs=10;, score=0.835 total time=   1.2s\n",
      "[CV 4/5; 1/9] START batch_size=10, epochs=10....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/9] END .....batch_size=10, epochs=10;, score=0.845 total time=   1.5s\n",
      "[CV 5/5; 1/9] START batch_size=10, epochs=10....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/9] END .....batch_size=10, epochs=10;, score=0.864 total time=   1.0s\n",
      "[CV 1/5; 2/9] START batch_size=10, epochs=50....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 2/9] END .....batch_size=10, epochs=50;, score=1.000 total time=   2.5s\n",
      "[CV 2/5; 2/9] START batch_size=10, epochs=50....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 2/9] END .....batch_size=10, epochs=50;, score=0.894 total time=   2.5s\n",
      "[CV 3/5; 2/9] START batch_size=10, epochs=50....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 2/9] END .....batch_size=10, epochs=50;, score=0.893 total time=   2.5s\n",
      "[CV 4/5; 2/9] START batch_size=10, epochs=50....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 2/9] END .....batch_size=10, epochs=50;, score=0.913 total time=   2.5s\n",
      "[CV 5/5; 2/9] START batch_size=10, epochs=50....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 2/9] END .....batch_size=10, epochs=50;, score=0.883 total time=   3.1s\n",
      "[CV 1/5; 3/9] START batch_size=10, epochs=100...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 3/9] END ....batch_size=10, epochs=100;, score=0.990 total time=   4.3s\n",
      "[CV 2/5; 3/9] START batch_size=10, epochs=100...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 3/9] END ....batch_size=10, epochs=100;, score=0.885 total time=   4.4s\n",
      "[CV 3/5; 3/9] START batch_size=10, epochs=100...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 3/9] END ....batch_size=10, epochs=100;, score=0.893 total time=   5.7s\n",
      "[CV 4/5; 3/9] START batch_size=10, epochs=100...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 3/9] END ....batch_size=10, epochs=100;, score=0.903 total time=   4.3s\n",
      "[CV 5/5; 3/9] START batch_size=10, epochs=100...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 3/9] END ....batch_size=10, epochs=100;, score=0.864 total time=   4.7s\n",
      "[CV 1/5; 4/9] START batch_size=20, epochs=10....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 4/9] END .....batch_size=20, epochs=10;, score=0.981 total time=   0.8s\n",
      "[CV 2/5; 4/9] START batch_size=20, epochs=10....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 4/9] END .....batch_size=20, epochs=10;, score=0.779 total time=   0.9s\n",
      "[CV 3/5; 4/9] START batch_size=20, epochs=10....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 4/9] END .....batch_size=20, epochs=10;, score=0.806 total time=   0.9s\n",
      "[CV 4/5; 4/9] START batch_size=20, epochs=10....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 4/9] END .....batch_size=20, epochs=10;, score=0.854 total time=   0.8s\n",
      "[CV 5/5; 4/9] START batch_size=20, epochs=10....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 4/9] END .....batch_size=20, epochs=10;, score=0.854 total time=   0.8s\n",
      "[CV 1/5; 5/9] START batch_size=20, epochs=50....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 5/9] END .....batch_size=20, epochs=50;, score=1.000 total time=   1.8s\n",
      "[CV 2/5; 5/9] START batch_size=20, epochs=50....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 5/9] END .....batch_size=20, epochs=50;, score=0.846 total time=   1.5s\n",
      "[CV 3/5; 5/9] START batch_size=20, epochs=50....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 5/9] END .....batch_size=20, epochs=50;, score=0.903 total time=   1.6s\n",
      "[CV 4/5; 5/9] START batch_size=20, epochs=50....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 5/9] END .....batch_size=20, epochs=50;, score=0.922 total time=   1.6s\n",
      "[CV 5/5; 5/9] START batch_size=20, epochs=50....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 5/9] END .....batch_size=20, epochs=50;, score=0.893 total time=   2.1s\n",
      "[CV 1/5; 6/9] START batch_size=20, epochs=100...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 6/9] END ....batch_size=20, epochs=100;, score=1.000 total time=   2.5s\n",
      "[CV 2/5; 6/9] START batch_size=20, epochs=100...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 6/9] END ....batch_size=20, epochs=100;, score=0.856 total time=   3.1s\n",
      "[CV 3/5; 6/9] START batch_size=20, epochs=100...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 6/9] END ....batch_size=20, epochs=100;, score=0.903 total time=   3.1s\n",
      "[CV 4/5; 6/9] START batch_size=20, epochs=100...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 6/9] END ....batch_size=20, epochs=100;, score=0.913 total time=   3.1s\n",
      "[CV 5/5; 6/9] START batch_size=20, epochs=100...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 6/9] END ....batch_size=20, epochs=100;, score=0.913 total time=   2.5s\n",
      "[CV 1/5; 7/9] START batch_size=40, epochs=10....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 7/9] END .....batch_size=40, epochs=10;, score=1.000 total time=   0.7s\n",
      "[CV 2/5; 7/9] START batch_size=40, epochs=10....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 7/9] END .....batch_size=40, epochs=10;, score=0.788 total time=   0.7s\n",
      "[CV 3/5; 7/9] START batch_size=40, epochs=10....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 7/9] END .....batch_size=40, epochs=10;, score=0.524 total time=   0.7s\n",
      "[CV 4/5; 7/9] START batch_size=40, epochs=10....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 7/9] END .....batch_size=40, epochs=10;, score=0.680 total time=   0.7s\n",
      "[CV 5/5; 7/9] START batch_size=40, epochs=10....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 7/9] END .....batch_size=40, epochs=10;, score=0.835 total time=   0.7s\n",
      "[CV 1/5; 8/9] START batch_size=40, epochs=50....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 8/9] END .....batch_size=40, epochs=50;, score=1.000 total time=   1.5s\n",
      "[CV 2/5; 8/9] START batch_size=40, epochs=50....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 8/9] END .....batch_size=40, epochs=50;, score=0.865 total time=   1.1s\n",
      "[CV 3/5; 8/9] START batch_size=40, epochs=50....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 8/9] END .....batch_size=40, epochs=50;, score=0.883 total time=   1.1s\n",
      "[CV 4/5; 8/9] START batch_size=40, epochs=50....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 8/9] END .....batch_size=40, epochs=50;, score=0.903 total time=   1.1s\n",
      "[CV 5/5; 8/9] START batch_size=40, epochs=50....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 8/9] END .....batch_size=40, epochs=50;, score=0.913 total time=   1.2s\n",
      "[CV 1/5; 9/9] START batch_size=40, epochs=100...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 9/9] END ....batch_size=40, epochs=100;, score=0.990 total time=   1.6s\n",
      "[CV 2/5; 9/9] START batch_size=40, epochs=100...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 9/9] END ....batch_size=40, epochs=100;, score=0.856 total time=   1.6s\n",
      "[CV 3/5; 9/9] START batch_size=40, epochs=100...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 9/9] END ....batch_size=40, epochs=100;, score=0.883 total time=   1.8s\n",
      "[CV 4/5; 9/9] START batch_size=40, epochs=100...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 9/9] END ....batch_size=40, epochs=100;, score=0.913 total time=   1.7s\n",
      "[CV 5/5; 9/9] START batch_size=40, epochs=100...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 9/9] END ....batch_size=40, epochs=100;, score=0.893 total time=   1.9s\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = KerasClassifier(build_fn = create_model,verbose = 0)\n",
    "# Define the grid search parameters\n",
    "batch_size = [10,20,40]\n",
    "epochs = [10,50,100]\n",
    "# Make a dictionary of the grid search parameters\n",
    "param_grid = dict(batch_size = batch_size,epochs = epochs)\n",
    "# Build and fit the GridSearchCV\n",
    "grid = GridSearchCV(estimator = model,param_grid = param_grid,verbose = 10)\n",
    "grid_result = grid.fit(X_standardized,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 456,
     "status": "ok",
     "timestamp": 1641643264159,
     "user": {
      "displayName": "Mohd Muhaz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00454428618184869141"
     },
     "user_tz": -330
    },
    "id": "vsAP_LydmEX9",
    "outputId": "6d46ffce-b3eb-4a65-8a57-360e2f46d4b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best : 0.916784918308258, using {'batch_size': 20, 'epochs': 100}\n",
      "0.8702763319015503,0.06736088961808087 with: {'batch_size': 10, 'epochs': 10}\n",
      "0.9167102336883545,0.042696653548278625 with: {'batch_size': 10, 'epochs': 50}\n",
      "0.9070388317108155,0.043596852318411664 with: {'batch_size': 10, 'epochs': 100}\n",
      "0.8548356890678406,0.06933415437437156 with: {'batch_size': 20, 'epochs': 10}\n",
      "0.912920081615448,0.05023558145164511 with: {'batch_size': 20, 'epochs': 50}\n",
      "0.916784918308258,0.046636416022829 with: {'batch_size': 20, 'epochs': 100}\n",
      "0.7654592990875244,0.15868762787312854 with: {'batch_size': 40, 'epochs': 10}\n",
      "0.9128827571868896,0.04649427631529634 with: {'batch_size': 40, 'epochs': 50}\n",
      "0.9070948481559753,0.045501554830199034 with: {'batch_size': 40, 'epochs': 100}\n"
     ]
    }
   ],
   "source": [
    "# Summarize the results\n",
    "print('Best : {}, using {}'.format(grid_result.best_score_,grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "  print('{},{} with: {}'.format(mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NahqdUl0kwmR"
   },
   "source": [
    "#**Tuning of Hyperparameters:- Learning rate and Drop out rate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35991,
     "status": "ok",
     "timestamp": 1641643639461,
     "user": {
      "displayName": "Mohd Muhaz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00454428618184869141"
     },
     "user_tz": -330
    },
    "id": "kVMiyHVHku7e",
    "outputId": "83270926-8f7f-48a6-c4bd-fd15e5519427"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "[CV 1/5; 1/9] START dropout_rate=0.0, learning_rate=0.001.......................\n",
      "[CV 1/5; 1/9] END dropout_rate=0.0, learning_rate=0.001;, score=1.000 total time=   0.7s\n",
      "[CV 2/5; 1/9] START dropout_rate=0.0, learning_rate=0.001.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/9] END dropout_rate=0.0, learning_rate=0.001;, score=0.750 total time=   0.7s\n",
      "[CV 3/5; 1/9] START dropout_rate=0.0, learning_rate=0.001.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/9] END dropout_rate=0.0, learning_rate=0.001;, score=0.524 total time=   0.7s\n",
      "[CV 4/5; 1/9] START dropout_rate=0.0, learning_rate=0.001.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/9] END dropout_rate=0.0, learning_rate=0.001;, score=0.680 total time=   0.8s\n",
      "[CV 5/5; 1/9] START dropout_rate=0.0, learning_rate=0.001.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/9] END dropout_rate=0.0, learning_rate=0.001;, score=0.709 total time=   0.7s\n",
      "[CV 1/5; 2/9] START dropout_rate=0.0, learning_rate=0.01........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 2/9] END dropout_rate=0.0, learning_rate=0.01;, score=0.981 total time=   0.7s\n",
      "[CV 2/5; 2/9] START dropout_rate=0.0, learning_rate=0.01........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 2/9] END dropout_rate=0.0, learning_rate=0.01;, score=0.769 total time=   0.7s\n",
      "[CV 3/5; 2/9] START dropout_rate=0.0, learning_rate=0.01........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 2/9] END dropout_rate=0.0, learning_rate=0.01;, score=0.524 total time=   0.7s\n",
      "[CV 4/5; 2/9] START dropout_rate=0.0, learning_rate=0.01........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 2/9] END dropout_rate=0.0, learning_rate=0.01;, score=0.825 total time=   0.7s\n",
      "[CV 5/5; 2/9] START dropout_rate=0.0, learning_rate=0.01........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 2/9] END dropout_rate=0.0, learning_rate=0.01;, score=0.845 total time=   1.1s\n",
      "[CV 1/5; 3/9] START dropout_rate=0.0, learning_rate=0.1.........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 3/9] END dropout_rate=0.0, learning_rate=0.1;, score=0.962 total time=   0.7s\n",
      "[CV 2/5; 3/9] START dropout_rate=0.0, learning_rate=0.1.........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 3/9] END dropout_rate=0.0, learning_rate=0.1;, score=0.760 total time=   0.7s\n",
      "[CV 3/5; 3/9] START dropout_rate=0.0, learning_rate=0.1.........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 3/9] END dropout_rate=0.0, learning_rate=0.1;, score=0.738 total time=   0.7s\n",
      "[CV 4/5; 3/9] START dropout_rate=0.0, learning_rate=0.1.........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 3/9] END dropout_rate=0.0, learning_rate=0.1;, score=0.767 total time=   0.7s\n",
      "[CV 5/5; 3/9] START dropout_rate=0.0, learning_rate=0.1.........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 3/9] END dropout_rate=0.0, learning_rate=0.1;, score=0.864 total time=   0.7s\n",
      "[CV 1/5; 4/9] START dropout_rate=0.1, learning_rate=0.001.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 4/9] END dropout_rate=0.1, learning_rate=0.001;, score=1.000 total time=   0.7s\n",
      "[CV 2/5; 4/9] START dropout_rate=0.1, learning_rate=0.001.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 4/9] END dropout_rate=0.1, learning_rate=0.001;, score=0.750 total time=   0.8s\n",
      "[CV 3/5; 4/9] START dropout_rate=0.1, learning_rate=0.001.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 4/9] END dropout_rate=0.1, learning_rate=0.001;, score=0.524 total time=   0.8s\n",
      "[CV 4/5; 4/9] START dropout_rate=0.1, learning_rate=0.001.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 4/9] END dropout_rate=0.1, learning_rate=0.001;, score=0.680 total time=   0.8s\n",
      "[CV 5/5; 4/9] START dropout_rate=0.1, learning_rate=0.001.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 4/9] END dropout_rate=0.1, learning_rate=0.001;, score=0.699 total time=   0.7s\n",
      "[CV 1/5; 5/9] START dropout_rate=0.1, learning_rate=0.01........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 5/9] END dropout_rate=0.1, learning_rate=0.01;, score=1.000 total time=   1.1s\n",
      "[CV 2/5; 5/9] START dropout_rate=0.1, learning_rate=0.01........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 5/9] END dropout_rate=0.1, learning_rate=0.01;, score=0.750 total time=   0.8s\n",
      "[CV 3/5; 5/9] START dropout_rate=0.1, learning_rate=0.01........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 5/9] END dropout_rate=0.1, learning_rate=0.01;, score=0.524 total time=   0.8s\n",
      "[CV 4/5; 5/9] START dropout_rate=0.1, learning_rate=0.01........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 5/9] END dropout_rate=0.1, learning_rate=0.01;, score=0.748 total time=   0.8s\n",
      "[CV 5/5; 5/9] START dropout_rate=0.1, learning_rate=0.01........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 5/9] END dropout_rate=0.1, learning_rate=0.01;, score=0.777 total time=   0.8s\n",
      "[CV 1/5; 6/9] START dropout_rate=0.1, learning_rate=0.1.........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 6/9] END dropout_rate=0.1, learning_rate=0.1;, score=0.846 total time=   0.8s\n",
      "[CV 2/5; 6/9] START dropout_rate=0.1, learning_rate=0.1.........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 6/9] END dropout_rate=0.1, learning_rate=0.1;, score=0.760 total time=   0.8s\n",
      "[CV 3/5; 6/9] START dropout_rate=0.1, learning_rate=0.1.........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 6/9] END dropout_rate=0.1, learning_rate=0.1;, score=0.524 total time=   0.8s\n",
      "[CV 4/5; 6/9] START dropout_rate=0.1, learning_rate=0.1.........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 6/9] END dropout_rate=0.1, learning_rate=0.1;, score=0.680 total time=   0.8s\n",
      "[CV 5/5; 6/9] START dropout_rate=0.1, learning_rate=0.1.........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 6/9] END dropout_rate=0.1, learning_rate=0.1;, score=0.777 total time=   0.7s\n",
      "[CV 1/5; 7/9] START dropout_rate=0.2, learning_rate=0.001.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 7/9] END dropout_rate=0.2, learning_rate=0.001;, score=1.000 total time=   1.1s\n",
      "[CV 2/5; 7/9] START dropout_rate=0.2, learning_rate=0.001.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 7/9] END dropout_rate=0.2, learning_rate=0.001;, score=0.750 total time=   0.8s\n",
      "[CV 3/5; 7/9] START dropout_rate=0.2, learning_rate=0.001.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 7/9] END dropout_rate=0.2, learning_rate=0.001;, score=0.524 total time=   0.8s\n",
      "[CV 4/5; 7/9] START dropout_rate=0.2, learning_rate=0.001.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 7/9] END dropout_rate=0.2, learning_rate=0.001;, score=0.680 total time=   0.8s\n",
      "[CV 5/5; 7/9] START dropout_rate=0.2, learning_rate=0.001.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 7/9] END dropout_rate=0.2, learning_rate=0.001;, score=0.699 total time=   0.7s\n",
      "[CV 1/5; 8/9] START dropout_rate=0.2, learning_rate=0.01........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 8/9] END dropout_rate=0.2, learning_rate=0.01;, score=0.990 total time=   0.8s\n",
      "[CV 2/5; 8/9] START dropout_rate=0.2, learning_rate=0.01........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 8/9] END dropout_rate=0.2, learning_rate=0.01;, score=0.750 total time=   0.8s\n",
      "[CV 3/5; 8/9] START dropout_rate=0.2, learning_rate=0.01........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 8/9] END dropout_rate=0.2, learning_rate=0.01;, score=0.524 total time=   0.8s\n",
      "[CV 4/5; 8/9] START dropout_rate=0.2, learning_rate=0.01........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 8/9] END dropout_rate=0.2, learning_rate=0.01;, score=0.786 total time=   0.8s\n",
      "[CV 5/5; 8/9] START dropout_rate=0.2, learning_rate=0.01........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 8/9] END dropout_rate=0.2, learning_rate=0.01;, score=0.777 total time=   0.7s\n",
      "[CV 1/5; 9/9] START dropout_rate=0.2, learning_rate=0.1.........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 9/9] END dropout_rate=0.2, learning_rate=0.1;, score=1.000 total time=   1.1s\n",
      "[CV 2/5; 9/9] START dropout_rate=0.2, learning_rate=0.1.........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 9/9] END dropout_rate=0.2, learning_rate=0.1;, score=0.750 total time=   0.7s\n",
      "[CV 3/5; 9/9] START dropout_rate=0.2, learning_rate=0.1.........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 9/9] END dropout_rate=0.2, learning_rate=0.1;, score=0.524 total time=   0.8s\n",
      "[CV 4/5; 9/9] START dropout_rate=0.2, learning_rate=0.1.........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 9/9] END dropout_rate=0.2, learning_rate=0.1;, score=0.680 total time=   0.8s\n",
      "[CV 5/5; 9/9] START dropout_rate=0.2, learning_rate=0.1.........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 9/9] END dropout_rate=0.2, learning_rate=0.1;, score=0.699 total time=   0.7s\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "# Defining the model\n",
    "\n",
    "def create_model(learning_rate,dropout_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8,input_dim = 28,kernel_initializer = 'normal',activation = 'relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(4,input_dim = 28,kernel_initializer = 'normal',activation = 'relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1,activation = 'sigmoid'))\n",
    "    \n",
    "    adam = Adam(lr = learning_rate)\n",
    "    model.compile(loss = 'binary_crossentropy',optimizer = adam,metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "\n",
    "model = KerasClassifier(build_fn = create_model,verbose = 0,batch_size = 40,epochs = 10)\n",
    "\n",
    "# Define the grid search parameters\n",
    "\n",
    "learning_rate = [0.001,0.01,0.1]\n",
    "dropout_rate = [0.0,0.1,0.2]\n",
    "\n",
    "# Make a dictionary of the grid search parameters\n",
    "\n",
    "param_grids = dict(learning_rate = learning_rate,dropout_rate = dropout_rate)\n",
    "\n",
    "# Build and fit the GridSearchCV\n",
    "\n",
    "grid = GridSearchCV(estimator = model,param_grid = param_grids,verbose = 10)\n",
    "grid_result = grid.fit(X_standardized,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 432,
     "status": "ok",
     "timestamp": 1641643669945,
     "user": {
      "displayName": "Mohd Muhaz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00454428618184869141"
     },
     "user_tz": -330
    },
    "id": "eBbuSmEEkvYp",
    "outputId": "92614c74-49ed-4ffa-f6ee-bec58b919181"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best : 0.8180171728134156, using {'dropout_rate': 0.0, 'learning_rate': 0.1}\n",
      "0.7325242638587952,0.15400213076804797 with: {'dropout_rate': 0.0, 'learning_rate': 0.001}\n",
      "0.7888349533081055,0.14943808583814033 with: {'dropout_rate': 0.0, 'learning_rate': 0.01}\n",
      "0.8180171728134156,0.08385577505466037 with: {'dropout_rate': 0.0, 'learning_rate': 0.1}\n",
      "0.7305825233459473,0.15435061319000673 with: {'dropout_rate': 0.1, 'learning_rate': 0.001}\n",
      "0.7597087383270263,0.15079829551524 with: {'dropout_rate': 0.1, 'learning_rate': 0.01}\n",
      "0.7172703385353089,0.11009307470874714 with: {'dropout_rate': 0.1, 'learning_rate': 0.1}\n",
      "0.7305825233459473,0.15435061319000673 with: {'dropout_rate': 0.2, 'learning_rate': 0.001}\n",
      "0.7655526518821716,0.1480316936319614 with: {'dropout_rate': 0.2, 'learning_rate': 0.01}\n",
      "0.7305825233459473,0.15435061319000673 with: {'dropout_rate': 0.2, 'learning_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Summarize the results\n",
    "print('Best : {}, using {}'.format(grid_result.best_score_,grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "  print('{},{} with: {}'.format(mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjXH1OQop6R0"
   },
   "source": [
    "#**Tuning of Hyperparameters:- Activation Function and Kernel Initializer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48467,
     "status": "ok",
     "timestamp": 1641643753857,
     "user": {
      "displayName": "Mohd Muhaz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00454428618184869141"
     },
     "user_tz": -330
    },
    "id": "LdWKQWeFqEV1",
    "outputId": "794a786f-eab7-4a54-8719-8059014594b7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[CV 1/5; 1/12] START activation_function=softmax, init=uniform..................\n",
      "[CV 1/5; 1/12] END activation_function=softmax, init=uniform;, score=1.000 total time=   0.8s\n",
      "[CV 2/5; 1/12] START activation_function=softmax, init=uniform..................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/12] END activation_function=softmax, init=uniform;, score=0.750 total time=   0.8s\n",
      "[CV 3/5; 1/12] START activation_function=softmax, init=uniform..................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/12] END activation_function=softmax, init=uniform;, score=0.524 total time=   0.8s\n",
      "[CV 4/5; 1/12] START activation_function=softmax, init=uniform..................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/12] END activation_function=softmax, init=uniform;, score=0.680 total time=   0.8s\n",
      "[CV 5/5; 1/12] START activation_function=softmax, init=uniform..................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/12] END activation_function=softmax, init=uniform;, score=0.699 total time=   1.3s\n",
      "[CV 1/5; 2/12] START activation_function=softmax, init=normal...................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 2/12] END activation_function=softmax, init=normal;, score=1.000 total time=   0.8s\n",
      "[CV 2/5; 2/12] START activation_function=softmax, init=normal...................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 2/12] END activation_function=softmax, init=normal;, score=0.750 total time=   0.8s\n",
      "[CV 3/5; 2/12] START activation_function=softmax, init=normal...................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 2/12] END activation_function=softmax, init=normal;, score=0.524 total time=   0.8s\n",
      "[CV 4/5; 2/12] START activation_function=softmax, init=normal...................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 2/12] END activation_function=softmax, init=normal;, score=0.680 total time=   0.8s\n",
      "[CV 5/5; 2/12] START activation_function=softmax, init=normal...................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 2/12] END activation_function=softmax, init=normal;, score=0.699 total time=   0.7s\n",
      "[CV 1/5; 3/12] START activation_function=softmax, init=zero.....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 3/12] END activation_function=softmax, init=zero;, score=1.000 total time=   0.8s\n",
      "[CV 2/5; 3/12] START activation_function=softmax, init=zero.....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 3/12] END activation_function=softmax, init=zero;, score=0.750 total time=   0.8s\n",
      "[CV 3/5; 3/12] START activation_function=softmax, init=zero.....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 3/12] END activation_function=softmax, init=zero;, score=0.524 total time=   0.8s\n",
      "[CV 4/5; 3/12] START activation_function=softmax, init=zero.....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 3/12] END activation_function=softmax, init=zero;, score=0.680 total time=   0.8s\n",
      "[CV 5/5; 3/12] START activation_function=softmax, init=zero.....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 3/12] END activation_function=softmax, init=zero;, score=0.699 total time=   1.1s\n",
      "[CV 1/5; 4/12] START activation_function=relu, init=uniform.....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 4/12] END activation_function=relu, init=uniform;, score=1.000 total time=   0.7s\n",
      "[CV 2/5; 4/12] START activation_function=relu, init=uniform.....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 4/12] END activation_function=relu, init=uniform;, score=0.750 total time=   0.7s\n",
      "[CV 3/5; 4/12] START activation_function=relu, init=uniform.....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 4/12] END activation_function=relu, init=uniform;, score=0.524 total time=   0.8s\n",
      "[CV 4/5; 4/12] START activation_function=relu, init=uniform.....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 4/12] END activation_function=relu, init=uniform;, score=0.680 total time=   0.8s\n",
      "[CV 5/5; 4/12] START activation_function=relu, init=uniform.....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 4/12] END activation_function=relu, init=uniform;, score=0.699 total time=   0.7s\n",
      "[CV 1/5; 5/12] START activation_function=relu, init=normal......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 5/12] END activation_function=relu, init=normal;, score=1.000 total time=   0.8s\n",
      "[CV 2/5; 5/12] START activation_function=relu, init=normal......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 5/12] END activation_function=relu, init=normal;, score=0.750 total time=   0.7s\n",
      "[CV 3/5; 5/12] START activation_function=relu, init=normal......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 5/12] END activation_function=relu, init=normal;, score=0.524 total time=   0.7s\n",
      "[CV 4/5; 5/12] START activation_function=relu, init=normal......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 5/12] END activation_function=relu, init=normal;, score=0.680 total time=   0.7s\n",
      "[CV 5/5; 5/12] START activation_function=relu, init=normal......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 5/12] END activation_function=relu, init=normal;, score=0.699 total time=   1.1s\n",
      "[CV 1/5; 6/12] START activation_function=relu, init=zero........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 6/12] END activation_function=relu, init=zero;, score=1.000 total time=   0.8s\n",
      "[CV 2/5; 6/12] START activation_function=relu, init=zero........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 6/12] END activation_function=relu, init=zero;, score=0.750 total time=   0.8s\n",
      "[CV 3/5; 6/12] START activation_function=relu, init=zero........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 6/12] END activation_function=relu, init=zero;, score=0.524 total time=   0.7s\n",
      "[CV 4/5; 6/12] START activation_function=relu, init=zero........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 6/12] END activation_function=relu, init=zero;, score=0.680 total time=   0.8s\n",
      "[CV 5/5; 6/12] START activation_function=relu, init=zero........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 6/12] END activation_function=relu, init=zero;, score=0.699 total time=   0.7s\n",
      "[CV 1/5; 7/12] START activation_function=tanh, init=uniform.....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 7/12] END activation_function=tanh, init=uniform;, score=0.962 total time=   0.7s\n",
      "[CV 2/5; 7/12] START activation_function=tanh, init=uniform.....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 7/12] END activation_function=tanh, init=uniform;, score=0.750 total time=   0.8s\n",
      "[CV 3/5; 7/12] START activation_function=tanh, init=uniform.....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 7/12] END activation_function=tanh, init=uniform;, score=0.583 total time=   0.8s\n",
      "[CV 4/5; 7/12] START activation_function=tanh, init=uniform.....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 7/12] END activation_function=tanh, init=uniform;, score=0.689 total time=   0.8s\n",
      "[CV 5/5; 7/12] START activation_function=tanh, init=uniform.....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 7/12] END activation_function=tanh, init=uniform;, score=0.680 total time=   1.1s\n",
      "[CV 1/5; 8/12] START activation_function=tanh, init=normal......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 8/12] END activation_function=tanh, init=normal;, score=0.971 total time=   0.8s\n",
      "[CV 2/5; 8/12] START activation_function=tanh, init=normal......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 8/12] END activation_function=tanh, init=normal;, score=0.750 total time=   0.8s\n",
      "[CV 3/5; 8/12] START activation_function=tanh, init=normal......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 8/12] END activation_function=tanh, init=normal;, score=0.573 total time=   0.8s\n",
      "[CV 4/5; 8/12] START activation_function=tanh, init=normal......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 8/12] END activation_function=tanh, init=normal;, score=0.670 total time=   0.7s\n",
      "[CV 5/5; 8/12] START activation_function=tanh, init=normal......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 8/12] END activation_function=tanh, init=normal;, score=0.728 total time=   0.8s\n",
      "[CV 1/5; 9/12] START activation_function=tanh, init=zero........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 9/12] END activation_function=tanh, init=zero;, score=1.000 total time=   0.8s\n",
      "[CV 2/5; 9/12] START activation_function=tanh, init=zero........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 9/12] END activation_function=tanh, init=zero;, score=0.750 total time=   0.8s\n",
      "[CV 3/5; 9/12] START activation_function=tanh, init=zero........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 9/12] END activation_function=tanh, init=zero;, score=0.524 total time=   0.8s\n",
      "[CV 4/5; 9/12] START activation_function=tanh, init=zero........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 9/12] END activation_function=tanh, init=zero;, score=0.680 total time=   0.7s\n",
      "[CV 5/5; 9/12] START activation_function=tanh, init=zero........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 9/12] END activation_function=tanh, init=zero;, score=0.699 total time=   1.2s\n",
      "[CV 1/5; 10/12] START activation_function=linear, init=uniform..................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 10/12] END activation_function=linear, init=uniform;, score=0.962 total time=   0.8s\n",
      "[CV 2/5; 10/12] START activation_function=linear, init=uniform..................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 10/12] END activation_function=linear, init=uniform;, score=0.750 total time=   0.8s\n",
      "[CV 3/5; 10/12] START activation_function=linear, init=uniform..................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 10/12] END activation_function=linear, init=uniform;, score=0.621 total time=   0.7s\n",
      "[CV 4/5; 10/12] START activation_function=linear, init=uniform..................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 10/12] END activation_function=linear, init=uniform;, score=0.680 total time=   0.7s\n",
      "[CV 5/5; 10/12] START activation_function=linear, init=uniform..................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 10/12] END activation_function=linear, init=uniform;, score=0.709 total time=   0.7s\n",
      "[CV 1/5; 11/12] START activation_function=linear, init=normal...................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 11/12] END activation_function=linear, init=normal;, score=0.990 total time=   0.8s\n",
      "[CV 2/5; 11/12] START activation_function=linear, init=normal...................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 11/12] END activation_function=linear, init=normal;, score=0.750 total time=   0.8s\n",
      "[CV 3/5; 11/12] START activation_function=linear, init=normal...................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 11/12] END activation_function=linear, init=normal;, score=0.612 total time=   0.7s\n",
      "[CV 4/5; 11/12] START activation_function=linear, init=normal...................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 11/12] END activation_function=linear, init=normal;, score=0.680 total time=   0.7s\n",
      "[CV 5/5; 11/12] START activation_function=linear, init=normal...................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 11/12] END activation_function=linear, init=normal;, score=0.728 total time=   1.1s\n",
      "[CV 1/5; 12/12] START activation_function=linear, init=zero.....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 12/12] END activation_function=linear, init=zero;, score=1.000 total time=   0.7s\n",
      "[CV 2/5; 12/12] START activation_function=linear, init=zero.....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 12/12] END activation_function=linear, init=zero;, score=0.750 total time=   0.8s\n",
      "[CV 3/5; 12/12] START activation_function=linear, init=zero.....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 12/12] END activation_function=linear, init=zero;, score=0.524 total time=   0.8s\n",
      "[CV 4/5; 12/12] START activation_function=linear, init=zero.....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 12/12] END activation_function=linear, init=zero;, score=0.680 total time=   0.7s\n",
      "[CV 5/5; 12/12] START activation_function=linear, init=zero.....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 12/12] END activation_function=linear, init=zero;, score=0.699 total time=   0.7s\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "# Defining the model\n",
    "\n",
    "def create_model(activation_function,init):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8,input_dim = 28,kernel_initializer = init,activation = activation_function))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(4,input_dim = 28,kernel_initializer = init,activation = activation_function))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(1,activation = 'sigmoid'))\n",
    "    \n",
    "    adam = Adam(lr = 0.001)\n",
    "    model.compile(loss = 'binary_crossentropy',optimizer = adam,metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "\n",
    "model = KerasClassifier(build_fn = create_model,verbose = 0,batch_size = 40,epochs = 10)\n",
    "\n",
    "# Define the grid search parameters\n",
    "activation_function = ['softmax','relu','tanh','linear']\n",
    "init = ['uniform','normal','zero']\n",
    "\n",
    "# Make a dictionary of the grid search parameters\n",
    "param_grids = dict(activation_function = activation_function,init = init)\n",
    "\n",
    "# Build and fit the GridSearchCV\n",
    "\n",
    "grid = GridSearchCV(estimator = model,param_grid = param_grids,verbose = 10)\n",
    "grid_result = grid.fit(X_standardized,Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 459,
     "status": "ok",
     "timestamp": 1641644130947,
     "user": {
      "displayName": "Mohd Muhaz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00454428618184869141"
     },
     "user_tz": -330
    },
    "id": "g6o4xYsIqEgR",
    "outputId": "42ede401-2081-421a-d902-8bc3660c7a63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best : 0.7519604086875915, using {'activation_function': 'linear', 'init': 'normal'}\n",
      "0.7305825233459473,0.15435061319000673 with: {'activation_function': 'softmax', 'init': 'uniform'}\n",
      "0.7305825233459473,0.15435061319000673 with: {'activation_function': 'softmax', 'init': 'normal'}\n",
      "0.7305825233459473,0.15435061319000673 with: {'activation_function': 'softmax', 'init': 'zero'}\n",
      "0.7305825233459473,0.15435061319000673 with: {'activation_function': 'relu', 'init': 'uniform'}\n",
      "0.7305825233459473,0.15435061319000673 with: {'activation_function': 'relu', 'init': 'normal'}\n",
      "0.7305825233459473,0.15435061319000673 with: {'activation_function': 'relu', 'init': 'zero'}\n",
      "0.7325989484786988,0.12642719289436974 with: {'activation_function': 'tanh', 'init': 'uniform'}\n",
      "0.7384055256843567,0.1315476603321602 with: {'activation_function': 'tanh', 'init': 'normal'}\n",
      "0.7305825233459473,0.15435061319000673 with: {'activation_function': 'tanh', 'init': 'zero'}\n",
      "0.744249427318573,0.11643770880999836 with: {'activation_function': 'linear', 'init': 'uniform'}\n",
      "0.7519604086875915,0.12832604215062365 with: {'activation_function': 'linear', 'init': 'normal'}\n",
      "0.7305825233459473,0.15435061319000673 with: {'activation_function': 'linear', 'init': 'zero'}\n"
     ]
    }
   ],
   "source": [
    "# Summarize the results\n",
    "print('Best : {}, using {}'.format(grid_result.best_score_,grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "  print('{},{} with: {}'.format(mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evr3SHwKqErD"
   },
   "source": [
    "#**Tuning of Hyperparameter :-Number of Neurons in activation layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37244,
     "status": "ok",
     "timestamp": 1641644224367,
     "user": {
      "displayName": "Mohd Muhaz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00454428618184869141"
     },
     "user_tz": -330
    },
    "id": "-zmDQcHmqEuX",
    "outputId": "ad329c0b-6f2e-4cbd-ad90-a5835f48faf4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "[CV 1/5; 1/9] START neuron1=4, neuron2=2........................................\n",
      "[CV 1/5; 1/9] END .........neuron1=4, neuron2=2;, score=0.971 total time=   0.8s\n",
      "[CV 2/5; 1/9] START neuron1=4, neuron2=2........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/9] END .........neuron1=4, neuron2=2;, score=0.750 total time=   0.8s\n",
      "[CV 3/5; 1/9] START neuron1=4, neuron2=2........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/9] END .........neuron1=4, neuron2=2;, score=0.592 total time=   0.7s\n",
      "[CV 4/5; 1/9] START neuron1=4, neuron2=2........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/9] END .........neuron1=4, neuron2=2;, score=0.680 total time=   0.7s\n",
      "[CV 5/5; 1/9] START neuron1=4, neuron2=2........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/9] END .........neuron1=4, neuron2=2;, score=0.718 total time=   1.1s\n",
      "[CV 1/5; 2/9] START neuron1=4, neuron2=4........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 2/9] END .........neuron1=4, neuron2=4;, score=0.952 total time=   0.8s\n",
      "[CV 2/5; 2/9] START neuron1=4, neuron2=4........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 2/9] END .........neuron1=4, neuron2=4;, score=0.750 total time=   0.7s\n",
      "[CV 3/5; 2/9] START neuron1=4, neuron2=4........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 2/9] END .........neuron1=4, neuron2=4;, score=0.573 total time=   0.7s\n",
      "[CV 4/5; 2/9] START neuron1=4, neuron2=4........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 2/9] END .........neuron1=4, neuron2=4;, score=0.680 total time=   0.8s\n",
      "[CV 5/5; 2/9] START neuron1=4, neuron2=4........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 2/9] END .........neuron1=4, neuron2=4;, score=0.728 total time=   0.7s\n",
      "[CV 1/5; 3/9] START neuron1=4, neuron2=8........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 3/9] END .........neuron1=4, neuron2=8;, score=0.962 total time=   0.8s\n",
      "[CV 2/5; 3/9] START neuron1=4, neuron2=8........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 3/9] END .........neuron1=4, neuron2=8;, score=0.750 total time=   0.8s\n",
      "[CV 3/5; 3/9] START neuron1=4, neuron2=8........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 3/9] END .........neuron1=4, neuron2=8;, score=0.573 total time=   0.7s\n",
      "[CV 4/5; 3/9] START neuron1=4, neuron2=8........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 3/9] END .........neuron1=4, neuron2=8;, score=0.680 total time=   0.7s\n",
      "[CV 5/5; 3/9] START neuron1=4, neuron2=8........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 3/9] END .........neuron1=4, neuron2=8;, score=0.709 total time=   1.2s\n",
      "[CV 1/5; 4/9] START neuron1=8, neuron2=2........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 4/9] END .........neuron1=8, neuron2=2;, score=0.962 total time=   0.8s\n",
      "[CV 2/5; 4/9] START neuron1=8, neuron2=2........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 4/9] END .........neuron1=8, neuron2=2;, score=0.750 total time=   0.8s\n",
      "[CV 3/5; 4/9] START neuron1=8, neuron2=2........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 4/9] END .........neuron1=8, neuron2=2;, score=0.544 total time=   0.8s\n",
      "[CV 4/5; 4/9] START neuron1=8, neuron2=2........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 4/9] END .........neuron1=8, neuron2=2;, score=0.689 total time=   0.7s\n",
      "[CV 5/5; 4/9] START neuron1=8, neuron2=2........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 4/9] END .........neuron1=8, neuron2=2;, score=0.689 total time=   0.8s\n",
      "[CV 1/5; 5/9] START neuron1=8, neuron2=4........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 5/9] END .........neuron1=8, neuron2=4;, score=0.962 total time=   0.8s\n",
      "[CV 2/5; 5/9] START neuron1=8, neuron2=4........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 5/9] END .........neuron1=8, neuron2=4;, score=0.750 total time=   0.8s\n",
      "[CV 3/5; 5/9] START neuron1=8, neuron2=4........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 5/9] END .........neuron1=8, neuron2=4;, score=0.631 total time=   0.7s\n",
      "[CV 4/5; 5/9] START neuron1=8, neuron2=4........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 5/9] END .........neuron1=8, neuron2=4;, score=0.689 total time=   0.8s\n",
      "[CV 5/5; 5/9] START neuron1=8, neuron2=4........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 5/9] END .........neuron1=8, neuron2=4;, score=0.680 total time=   1.2s\n",
      "[CV 1/5; 6/9] START neuron1=8, neuron2=8........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 6/9] END .........neuron1=8, neuron2=8;, score=0.962 total time=   0.8s\n",
      "[CV 2/5; 6/9] START neuron1=8, neuron2=8........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 6/9] END .........neuron1=8, neuron2=8;, score=0.750 total time=   0.8s\n",
      "[CV 3/5; 6/9] START neuron1=8, neuron2=8........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 6/9] END .........neuron1=8, neuron2=8;, score=0.631 total time=   0.7s\n",
      "[CV 4/5; 6/9] START neuron1=8, neuron2=8........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 6/9] END .........neuron1=8, neuron2=8;, score=0.689 total time=   0.7s\n",
      "[CV 5/5; 6/9] START neuron1=8, neuron2=8........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 6/9] END .........neuron1=8, neuron2=8;, score=0.718 total time=   0.7s\n",
      "[CV 1/5; 7/9] START neuron1=16, neuron2=2.......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 7/9] END ........neuron1=16, neuron2=2;, score=0.942 total time=   0.7s\n",
      "[CV 2/5; 7/9] START neuron1=16, neuron2=2.......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 7/9] END ........neuron1=16, neuron2=2;, score=0.750 total time=   0.8s\n",
      "[CV 3/5; 7/9] START neuron1=16, neuron2=2.......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 7/9] END ........neuron1=16, neuron2=2;, score=0.621 total time=   0.8s\n",
      "[CV 4/5; 7/9] START neuron1=16, neuron2=2.......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 7/9] END ........neuron1=16, neuron2=2;, score=0.689 total time=   0.8s\n",
      "[CV 5/5; 7/9] START neuron1=16, neuron2=2.......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 7/9] END ........neuron1=16, neuron2=2;, score=0.670 total time=   1.1s\n",
      "[CV 1/5; 8/9] START neuron1=16, neuron2=4.......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 8/9] END ........neuron1=16, neuron2=4;, score=0.981 total time=   0.8s\n",
      "[CV 2/5; 8/9] START neuron1=16, neuron2=4.......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 8/9] END ........neuron1=16, neuron2=4;, score=0.750 total time=   0.8s\n",
      "[CV 3/5; 8/9] START neuron1=16, neuron2=4.......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 8/9] END ........neuron1=16, neuron2=4;, score=0.650 total time=   0.7s\n",
      "[CV 4/5; 8/9] START neuron1=16, neuron2=4.......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 8/9] END ........neuron1=16, neuron2=4;, score=0.699 total time=   0.8s\n",
      "[CV 5/5; 8/9] START neuron1=16, neuron2=4.......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 8/9] END ........neuron1=16, neuron2=4;, score=0.689 total time=   0.7s\n",
      "[CV 1/5; 9/9] START neuron1=16, neuron2=8.......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 9/9] END ........neuron1=16, neuron2=8;, score=0.942 total time=   0.8s\n",
      "[CV 2/5; 9/9] START neuron1=16, neuron2=8.......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 9/9] END ........neuron1=16, neuron2=8;, score=0.750 total time=   0.7s\n",
      "[CV 3/5; 9/9] START neuron1=16, neuron2=8.......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 9/9] END ........neuron1=16, neuron2=8;, score=0.680 total time=   0.8s\n",
      "[CV 4/5; 9/9] START neuron1=16, neuron2=8.......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 9/9] END ........neuron1=16, neuron2=8;, score=0.699 total time=   0.7s\n",
      "[CV 5/5; 9/9] START neuron1=16, neuron2=8.......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 9/9] END ........neuron1=16, neuron2=8;, score=0.709 total time=   1.2s\n"
     ]
    }
   ],
   "source": [
    "# Defining the model\n",
    "\n",
    "def create_model(neuron1,neuron2):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neuron1,input_dim = 28,kernel_initializer = 'uniform',activation = 'tanh'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(neuron2,input_dim = neuron1,kernel_initializer = 'uniform',activation = 'tanh'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(1,activation = 'sigmoid'))\n",
    "    \n",
    "    adam = Adam(lr = 0.001)\n",
    "    model.compile(loss = 'binary_crossentropy',optimizer = adam,metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "\n",
    "model = KerasClassifier(build_fn = create_model,verbose = 0,batch_size = 40,epochs = 10)\n",
    "\n",
    "# Define the grid search parameters\n",
    "\n",
    "neuron1 = [4,8,16]\n",
    "neuron2 = [2,4,8]\n",
    "\n",
    "# Make a dictionary of the grid search parameters\n",
    "\n",
    "param_grids = dict(neuron1 = neuron1,neuron2 = neuron2)\n",
    "\n",
    "# Build and fit the GridSearchCV\n",
    "\n",
    "grid = GridSearchCV(estimator = model,param_grid = param_grids,verbose = 10)\n",
    "grid_result = grid.fit(X_standardized,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 458,
     "status": "ok",
     "timestamp": 1641644238814,
     "user": {
      "displayName": "Mohd Muhaz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00454428618184869141"
     },
     "user_tz": -330
    },
    "id": "rdK4TZqhqEx0",
    "outputId": "d47c5369-8753-4c7f-f449-92d769e223c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best : 0.7559372663497925, using {'neuron1': 16, 'neuron2': 8}\n",
      "0.7422890186309814,0.1260573655154288 with: {'neuron1': 4, 'neuron2': 2}\n",
      "0.7365011096000671,0.12384496319422442 with: {'neuron1': 4, 'neuron2': 4}\n",
      "0.7345406889915467,0.12775365351992884 with: {'neuron1': 4, 'neuron2': 8}\n",
      "0.7267737030982971,0.13563136551395388 with: {'neuron1': 8, 'neuron2': 2}\n",
      "0.7423076748847961,0.1159603603149551 with: {'neuron1': 8, 'neuron2': 4}\n",
      "0.7500746726989747,0.11275721703342867 with: {'neuron1': 8, 'neuron2': 8}\n",
      "0.7345780491828918,0.11174889566369395 with: {'neuron1': 16, 'neuron2': 2}\n",
      "0.7539208412170411,0.11778009545312698 with: {'neuron1': 16, 'neuron2': 4}\n",
      "0.7559372663497925,0.0959803274542037 with: {'neuron1': 16, 'neuron2': 8}\n"
     ]
    }
   ],
   "source": [
    "# Summarize the results\n",
    "print('Best : {}, using {}'.format(grid_result.best_score_,grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "  print('{},{} with: {}'.format(mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOoZQjZvqE1J"
   },
   "source": [
    "#**Training model with optimum values of Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1217,
     "status": "ok",
     "timestamp": 1641644490677,
     "user": {
      "displayName": "Mohd Muhaz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00454428618184869141"
     },
     "user_tz": -330
    },
    "id": "T1A6No14qE4O",
    "outputId": "44ad8444-ddbc-4ddf-b2eb-0f757b85b034"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7794970986460348\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Defining the model\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16,input_dim = 28,kernel_initializer = 'uniform',activation = 'tanh'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(4,input_dim = 16,kernel_initializer = 'uniform',activation = 'tanh'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(1,activation = 'sigmoid'))\n",
    "    \n",
    "    adam = Adam(lr = 0.001) #sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "    model.compile(loss = 'binary_crossentropy',optimizer = adam,metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "\n",
    "model = KerasClassifier(build_fn = create_model,verbose = 0,batch_size = 40,epochs = 10)\n",
    "\n",
    "# Fitting the model\n",
    "\n",
    "model.fit(X_standardized,Y)\n",
    "\n",
    "# Predicting using trained model\n",
    "\n",
    "y_predict = model.predict(X_standardized)\n",
    "\n",
    "# Printing the metrics\n",
    "print(accuracy_score(Y,y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovOXHc9Jyo_v"
   },
   "source": [
    "#**Hyperparameters all at once**\n",
    "The hyperparameter optimization was carried out by taking 2 hyperparameters at once. We may have missed the best values. The performance can be further improved by finding the optimum values of hyperparameters all at once given by the code snippet below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "executionInfo": {
     "elapsed": 456,
     "status": "ok",
     "timestamp": 1641644639662,
     "user": {
      "displayName": "Mohd Muhaz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00454428618184869141"
     },
     "user_tz": -330
    },
    "id": "UBhchS8I04wq"
   },
   "outputs": [],
   "source": [
    "def create_model(learning_rate,dropout_rate,activation_function,init,neuron1,neuron2):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neuron1,input_dim = 28,kernel_initializer = init,activation = activation_function))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(neuron2,input_dim = neuron1,kernel_initializer = init,activation = activation_function))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1,activation = 'sigmoid'))\n",
    "    \n",
    "    adam = Adam(lr = learning_rate)\n",
    "    model.compile(loss = 'binary_crossentropy',optimizer = adam,metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 446,
     "status": "ok",
     "timestamp": 1641644665579,
     "user": {
      "displayName": "Mohd Muhaz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00454428618184869141"
     },
     "user_tz": -330
    },
    "id": "IRbd-Xv_1vB8",
    "outputId": "e7c42219-ec0c-424b-a5a4-c1cd97328ffa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "\n",
    "model = KerasClassifier(build_fn = create_model,verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "executionInfo": {
     "elapsed": 447,
     "status": "ok",
     "timestamp": 1641644689029,
     "user": {
      "displayName": "Mohd Muhaz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00454428618184869141"
     },
     "user_tz": -330
    },
    "id": "dqyWVBwHpaZ_"
   },
   "outputs": [],
   "source": [
    "# Define the grid search parameters\n",
    "\n",
    "batch_size = [10,20,40]\n",
    "epochs = [10,50,100]\n",
    "learning_rate = [0.001,0.01,0.1]\n",
    "dropout_rate = [0.0,0.1,0.2]\n",
    "activation_function = ['softmax','relu','tanh','linear']\n",
    "init = ['uniform','normal','zero']\n",
    "neuron1 = [4,8,16]\n",
    "neuron2 = [2,4,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "executionInfo": {
     "elapsed": 464,
     "status": "ok",
     "timestamp": 1641644705199,
     "user": {
      "displayName": "Mohd Muhaz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00454428618184869141"
     },
     "user_tz": -330
    },
    "id": "Zdy6BcpxpgI5"
   },
   "outputs": [],
   "source": [
    "# Make a dictionary of the grid search parameters\n",
    "\n",
    "param_grids = dict(batch_size = batch_size,epochs = epochs,learning_rate = learning_rate,dropout_rate = dropout_rate,\n",
    "                   activation_function = activation_function,init = init,neuron1 = neuron1,neuron2 = neuron2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 35450,
     "status": "error",
     "timestamp": 1641653736472,
     "user": {
      "displayName": "Mohd Muhaz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00454428618184869141"
     },
     "user_tz": -330
    },
    "id": "OKaJHlyMpzOB",
    "outputId": "fd3d8bbb-f3ed-4bb8-8983-e22ef34bc6dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8748 candidates, totalling 43740 fits\n",
      "[CV 1/5; 1/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2;, score=1.000 total time=   1.3s\n",
      "[CV 2/5; 1/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2;, score=0.750 total time=   1.0s\n",
      "[CV 3/5; 1/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2;, score=0.524 total time=   1.0s\n",
      "[CV 4/5; 1/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2;, score=0.680 total time=   1.0s\n",
      "[CV 5/5; 1/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2;, score=0.699 total time=   1.2s\n",
      "[CV 1/5; 2/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 2/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4;, score=1.000 total time=   1.4s\n",
      "[CV 2/5; 2/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 2/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4;, score=0.750 total time=   1.0s\n",
      "[CV 3/5; 2/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 2/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4;, score=0.524 total time=   1.0s\n",
      "[CV 4/5; 2/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 2/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4;, score=0.680 total time=   1.0s\n",
      "[CV 5/5; 2/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 2/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4;, score=0.699 total time=   1.2s\n",
      "[CV 1/5; 3/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 3/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8;, score=1.000 total time=   1.0s\n",
      "[CV 2/5; 3/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 3/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8;, score=0.750 total time=   1.3s\n",
      "[CV 3/5; 3/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 3/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8;, score=0.524 total time=   1.0s\n",
      "[CV 4/5; 3/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 3/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8;, score=0.680 total time=   1.0s\n",
      "[CV 5/5; 3/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 3/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8;, score=0.699 total time=   1.3s\n",
      "[CV 1/5; 4/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 4/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2;, score=1.000 total time=   1.2s\n",
      "[CV 2/5; 4/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 4/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2;, score=0.750 total time=   1.2s\n",
      "[CV 3/5; 4/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 4/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2;, score=0.524 total time=   1.2s\n",
      "[CV 4/5; 4/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 4/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2;, score=0.680 total time=   1.6s\n",
      "[CV 5/5; 4/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 4/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2;, score=0.699 total time=   1.3s\n",
      "[CV 1/5; 5/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 5/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4;, score=1.000 total time=   1.0s\n",
      "[CV 2/5; 5/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 5/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4;, score=0.750 total time=   1.0s\n",
      "[CV 3/5; 5/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 5/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4;, score=0.524 total time=   1.0s\n",
      "[CV 4/5; 5/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 5/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4;, score=0.680 total time=   1.2s\n",
      "[CV 5/5; 5/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 5/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4;, score=0.699 total time=   1.2s\n",
      "[CV 1/5; 6/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 6/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8;, score=1.000 total time=   1.2s\n",
      "[CV 2/5; 6/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 6/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8;, score=0.750 total time=   1.2s\n",
      "[CV 3/5; 6/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 6/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8;, score=0.524 total time=   1.2s\n",
      "[CV 4/5; 6/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 6/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8;, score=0.680 total time=   1.2s\n",
      "[CV 5/5; 6/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-026cadddf3d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_grids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgrid_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_standardized\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Summarize the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    849\u001b[0m                     )\n\u001b[1;32m    850\u001b[0m                     for (cand_idx, parameters), (split_idx, (train, test)) in product(\n\u001b[0;32m--> 851\u001b[0;31m                         \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m                     )\n\u001b[1;32m    853\u001b[0m                 )\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1044\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0mfit_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mtest_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m         \u001b[0mscore_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfit_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_score\u001b[0;34m(estimator, X_test, y_test, scorer, error_score)\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merror_score\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raise\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36m_passthrough_scorer\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_passthrough_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0;34m\"\"\"Function that wraps estimator.score\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m       \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1537\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1538\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    956\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 958\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    959\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    779\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    780\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 781\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3155\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3156\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3157\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3158\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3556\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3557\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3558\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3400\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3401\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3402\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3403\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3404\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1123\u001b[0m                     \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m                     \u001b[0moptional_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautograph_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1125\u001b[0;31m                     \u001b[0muser_requested\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1126\u001b[0m                 ))\n\u001b[1;32m   1127\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__test_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_allowlisted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m   \u001b[0;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    463\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mstep_function\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m   1354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m       \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m       outputs = reduce_per_replica(\n\u001b[1;32m   1358\u001b[0m           outputs, self.distribute_strategy, reduction='first')\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1314\u001b[0m       fn = autograph.tf_convert(\n\u001b[1;32m   1315\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[0;32m-> 1316\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2890\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2891\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2892\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2894\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3693\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3694\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mReplicaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplica_id_in_sync_group\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3695\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3697\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_reduce_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    694\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_allowlisted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m   \u001b[0;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1350\u001b[0m         \u001b[0;31m# Ensure counter is updated only if `test_step` succeeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_minimum_control_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtest_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1305\u001b[0m     self.compiled_loss(\n\u001b[1;32m   1306\u001b[0m         y, y_pred, sample_weight, regularization_losses=self.losses)\n\u001b[0;32m-> 1307\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiled_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1308\u001b[0m     \u001b[0;31m# Collect metrics to return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0mreturn_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py\u001b[0m in \u001b[0;36mupdate_state\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmetric_obj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m           \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0mmetric_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mweighted_metric_obj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweighted_metric_objs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/metrics_utils.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(metric_obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_context_for_symbolic_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m       \u001b[0mupdate_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_state_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mupdate_op\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# update_op will be None in eager execution.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0mmetric_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/metrics.py\u001b[0m in \u001b[0;36mupdate_state_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mcontrol_status\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mag_update_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_update_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol_status\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mag_update_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    694\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    335\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_in_allowlist_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Allowlisted %s: from cache'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/metrics.py\u001b[0m in \u001b[0;36mupdate_state\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m     return super(MeanMetricWrapper, self).update_state(\n\u001b[0;32m--> 727\u001b[0;31m         matches, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/metrics.py\u001b[0m in \u001b[0;36mupdate_state\u001b[0;34m(self, values, sample_weight)\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmetrics_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWEIGHTED_MEAN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m         \u001b[0mnum_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0mnum_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1094\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36msize_v2\u001b[0;34m(input, out_type, name)\u001b[0m\n\u001b[1;32m    732\u001b[0m   \"\"\"\n\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1094\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36msize\u001b[0;34m(input, name, out_type)\u001b[0m\n\u001b[1;32m    764\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mend_compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m   \"\"\"\n\u001b[0;32m--> 766\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msize_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36msize_internal\u001b[0;34m(input, name, optimize, out_type)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36msize\u001b[0;34m(input, out_type, name)\u001b[0m\n\u001b[1;32m   9508\u001b[0m   \u001b[0mout_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"out_type\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9509\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0;32m-> 9510\u001b[0;31m         \"Size\", input=input, out_type=out_type, name=name)\n\u001b[0m\u001b[1;32m   9511\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9512\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    744\u001b[0m       op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n\u001b[1;32m    745\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m                                  attrs=attr_protos, op_def=op_def)\n\u001b[0m\u001b[1;32m    747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m     \u001b[0;31m# `outputs` is returned as a separate return value so that the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m    689\u001b[0m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[1;32m    690\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m         compute_device)\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3703\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3704\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3705\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3706\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3707\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   2096\u001b[0m         \u001b[0mop_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_op_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2097\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, inputs,\n\u001b[0;32m-> 2098\u001b[0;31m                                 control_input_ops, op_def)\n\u001b[0m\u001b[1;32m   2099\u001b[0m       \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[1;32m   1934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1935\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1936\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1937\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1938\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Build and fit the GridSearchCV\n",
    "\n",
    "grid = GridSearchCV(estimator = model,param_grid = param_grids,verbose = 10)\n",
    "grid_result = grid.fit(X_standardized,Y)\n",
    "\n",
    "# Summarize the results\n",
    "print('Best : {}, using {}'.format(grid_result.best_score_,grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "  print('{},{} with: {}'.format(mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qmothJ0Ep8nF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNHNCtbvC3YkwUG+2GCF+OH",
   "name": "Assignment_16.1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
